{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Obtain dataset\n",
    "trainset = datasets.CIFAR10('./../0. Data/',\n",
    "                               download = True,\n",
    "                               train = True,\n",
    "                               transform = T.Compose([\n",
    "                                   T.ToTensor()\n",
    "                               ]))\n",
    "\n",
    "testset = datasets.CIFAR10('./../0. Data/',\n",
    "                              download = True,\n",
    "                              train = False,\n",
    "                              transform = T.Compose([\n",
    "                                  T.ToTensor()\n",
    "                              ]))\n",
    "\n",
    "#split training data further to training and validation data\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "\n",
    "#Put in dataloader\n",
    "train_loader_full = DataLoader(trainset,\n",
    "                           batch_size=32,\n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                       batch_size = 32,\n",
    "                       shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                        batch_size = 32,\n",
    "                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#check for GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a MLP class defining our neural network\n",
    "class vanilla_CNN(nn.Module):\n",
    "    def __init__(self, input_len, output_len):\n",
    "        super(vanilla_CNN, self).__init__()\n",
    "        \n",
    "        #Convvolutional and max pool layers\n",
    "        self.conv2d_32 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_1 = nn.BatchNorm2d(32)\n",
    "        self.conv2d_64 =  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_2 = nn.BatchNorm2d(64)\n",
    "        self.maxp2d_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2d_128 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_3 = nn.BatchNorm2d(128)\n",
    "        self.maxp2d_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2d_256 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_4 = nn.BatchNorm2d(256)\n",
    "        self.maxp2d_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #three fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=256*4*4, out_features=512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,3,32,32)\n",
    "        #input_size = 1x28x28, output_size = 32x28x28\n",
    "        x = F.relu(self.conv2d_32(x))\n",
    "        #input_size = 32x28x28, output_size = 32x28x28\n",
    "        x = self.batch2d_1(x)\n",
    "        #input_size = 32x28x28, output_size = 64x28x28\n",
    "        x = F.relu(self.conv2d_64(x))\n",
    "        #input_size = 64x28x28, output_size = 64x28x28\n",
    "        x = self.batch2d_2(x)\n",
    "        #input_size = 64x28x28, output_size = 64x15x15\n",
    "        x = F.relu(self.maxp2d_1(x))\n",
    "        #input_size = 64x15x15, output_size = 128x15x15\n",
    "        x = F.relu(self.conv2d_128(x))\n",
    "        #input_size = 128x15x15, output_size = 128x15x15\n",
    "        x = self.batch2d_3(x)\n",
    "        #input_size = 128x15x15, output_size = 128x7x7\n",
    "        x = F.relu(self.maxp2d_2(x))\n",
    "        #input_size = 128x7x7, output_size = 256x7x7\n",
    "        x = F.relu(self.conv2d_256(x))\n",
    "        #input_size = 256x7x7, output_size = 256x7x7\n",
    "        x = self.batch2d_4(x)\n",
    "        #input_size = 256x7x7, output_size = 256x3x3\n",
    "        x = F.relu(self.maxp2d_3(x))\n",
    "\n",
    "        #convert image to a one dimentional tensor before feeding to neural network\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, validate, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = train_loader_full\n",
    "        \n",
    "        if(validate):\n",
    "            loader = train_loader\n",
    "        \n",
    "        cnf_tr = torch.zeros(10,10)\n",
    "        cnf_val = torch.zeros(10,10)\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "\n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradiuents to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].to(device)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #calculate output by argmax\n",
    "                output = torch.argmax(output, 1)\n",
    "                #update entries in confusion matrix\n",
    "                for i in range(output.shape[0]):\n",
    "                    cnf_tr[output[i],sample[1][i]] +=1\n",
    "            \n",
    "        if(validate):\n",
    "            #Evaluate on validation data\n",
    "            with torch.no_grad():\n",
    "                #set model to evaluation mode\n",
    "                Model.eval()\n",
    "                #evaluate on tvaidation data\n",
    "                for i, sample in enumerate(val_loader):\n",
    "                    output = Model(sample[0].to(device))\n",
    "                    loss = loss_function(output, sample[1].to(device))\n",
    "                    Val_Loss.append(loss.item())\n",
    "                    #calculate output by argmax\n",
    "                    output = torch.argmax(output, 1)\n",
    "                    #update entries in confusion matrix\n",
    "                    for i in range(output.shape[0]):\n",
    "                        cnf_val[output[i],sample[1][i]] +=1\n",
    "                   \n",
    "        actual_count = torch.sum(cnf_tr, dim=0)\n",
    "        correct_pred = torch.tensor([cnf_tr[i,i] for i in range(10)])\n",
    "        A_tr = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        if(validate):\n",
    "            actual_count = torch.sum(cnf_val, dim=0)\n",
    "            correct_pred = torch.tensor([cnf_val[i,i] for i in range(10)])\n",
    "            A_val = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        #print losses in every epoch\n",
    "        if(validate):\n",
    "            print('epoch : ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Val_acc : ', np.round(A_val,4),  \n",
    "                  '; Train_loss  ',np.round(np.mean(Train_Loss),4),  '; Val_loss  ',np.round(np.mean(Val_Loss),4))\n",
    "        else:\n",
    "            print('epoch = ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function top evaluate model using performace metrices\n",
    "def evaluate(cnf):\n",
    "    actual_count = torch.sum(cnf, dim=0)\n",
    "    predicted_count = torch.sum(cnf, dim=1)\n",
    "    correct_pred = torch.tensor([cnf[i,i] for i in range(10)])\n",
    "    #Precision\n",
    "    precision = correct_pred/predicted_count\n",
    "    #Recall\n",
    "    recall = correct_pred/actual_count\n",
    "    #F1-Score\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    #Accuracy\n",
    "    Accuracy = torch.sum(correct_pred)/torch.sum(actual_count)\n",
    "    print('\\n',pd.DataFrame({'Class':[i for i in range(10)],\n",
    "                 'Precision' : precision,\n",
    "                 'Recall' : recall,\n",
    "                 'F1_Score': f1_score}))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy  : ', Accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model):\n",
    "    Loss = []\n",
    "    #confusion matrix\n",
    "    cnf = torch.zeros(10,10)\n",
    "\n",
    "    #evaluate on test data\n",
    "    with torch.no_grad():\n",
    "        #set model to evaluation mode\n",
    "        Model.eval()\n",
    "        #evaluate on test data\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            output = Model(sample[0].to(device))\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            Loss.append(loss.item())\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #update entries in confusion matrix\n",
    "            for i in range(output.shape[0]):\n",
    "                cnf[output[i],sample[1][i]] +=1\n",
    "\n",
    "        #print test loss\n",
    "        print('Test loss : ', np.mean(Loss))\n",
    "\n",
    "    #print evaluation summary\n",
    "    evaluate(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.5553 ; Val_acc :  0.6479 ; Train_loss   1.2407 ; Val_loss   1.0125\n",
      "epoch :  1 ; Train_acc :  0.7149 ; Val_acc :  0.7424 ; Train_loss   0.8207 ; Val_loss   0.7603\n",
      "epoch :  2 ; Train_acc :  0.7622 ; Val_acc :  0.6943 ; Train_loss   0.6898 ; Val_loss   0.876\n",
      "epoch :  3 ; Train_acc :  0.8023 ; Val_acc :  0.7899 ; Train_loss   0.5726 ; Val_loss   0.6343\n",
      "epoch :  4 ; Train_acc :  0.8413 ; Val_acc :  0.7893 ; Train_loss   0.4647 ; Val_loss   0.6352\n",
      "epoch :  5 ; Train_acc :  0.8622 ; Val_acc :  0.8016 ; Train_loss   0.3974 ; Val_loss   0.6364\n",
      "epoch :  6 ; Train_acc :  0.8905 ; Val_acc :  0.789 ; Train_loss   0.3163 ; Val_loss   0.7497\n",
      "epoch :  7 ; Train_acc :  0.9075 ; Val_acc :  0.7119 ; Train_loss   0.2687 ; Val_loss   1.1045\n",
      "epoch :  8 ; Train_acc :  0.9128 ; Val_acc :  0.7953 ; Train_loss   0.249 ; Val_loss   0.7921\n",
      "epoch :  9 ; Train_acc :  0.9381 ; Val_acc :  0.7992 ; Train_loss   0.1778 ; Val_loss   0.7935\n",
      "epoch :  10 ; Train_acc :  0.9427 ; Val_acc :  0.7963 ; Train_loss   0.1627 ; Val_loss   0.7997\n",
      "epoch :  11 ; Train_acc :  0.9515 ; Val_acc :  0.792 ; Train_loss   0.1403 ; Val_loss   1.0336\n",
      "epoch :  12 ; Train_acc :  0.9572 ; Val_acc :  0.795 ; Train_loss   0.1252 ; Val_loss   0.9091\n",
      "epoch :  13 ; Train_acc :  0.9599 ; Val_acc :  0.795 ; Train_loss   0.1148 ; Val_loss   0.872\n",
      "epoch :  14 ; Train_acc :  0.9598 ; Val_acc :  0.7995 ; Train_loss   0.1143 ; Val_loss   1.8036\n",
      "epoch :  15 ; Train_acc :  0.966 ; Val_acc :  0.8026 ; Train_loss   0.1009 ; Val_loss   1.1241\n",
      "epoch :  16 ; Train_acc :  0.9679 ; Val_acc :  0.7996 ; Train_loss   0.0908 ; Val_loss   0.8865\n",
      "epoch :  17 ; Train_acc :  0.9698 ; Val_acc :  0.806 ; Train_loss   0.0862 ; Val_loss   0.923\n",
      "epoch :  18 ; Train_acc :  0.9698 ; Val_acc :  0.8023 ; Train_loss   0.0862 ; Val_loss   0.9006\n",
      "epoch :  19 ; Train_acc :  0.9709 ; Val_acc :  0.8057 ; Train_loss   0.0836 ; Val_loss   0.9023\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = vanilla_CNN(1024,10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, validate=True, max_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.5633 ; Train_loss   1.2346\n",
      "epoch =  1 ; Train_acc :  0.7041 ; Train_loss   0.8594\n",
      "epoch =  2 ; Train_acc :  0.7539 ; Train_loss   0.7184\n",
      "epoch =  3 ; Train_acc :  0.8041 ; Train_loss   0.5735\n",
      "epoch =  4 ; Train_acc :  0.8328 ; Train_loss   0.4861\n",
      "epoch =  5 ; Train_acc :  0.859 ; Train_loss   0.4128\n",
      "epoch =  6 ; Train_acc :  0.8897 ; Train_loss   0.3255\n",
      "epoch =  7 ; Train_acc :  0.9087 ; Train_loss   0.2666\n",
      "epoch =  8 ; Train_acc :  0.9138 ; Train_loss   0.2486\n",
      "epoch =  9 ; Train_acc :  0.9203 ; Train_loss   0.233\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 10 epochs on full training set\n",
    "#Create Model\n",
    "Model = vanilla_CNN(1024,10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, validate=False, max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.6614584162974129\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.847890   0.864  0.855869\n",
      "1      1   0.914286   0.896  0.905051\n",
      "2      2   0.775269   0.721  0.747150\n",
      "3      3   0.632727   0.696  0.662857\n",
      "4      4   0.743847   0.816  0.778255\n",
      "5      5   0.742976   0.714  0.728200\n",
      "6      6   0.827947   0.871  0.848928\n",
      "7      7   0.914481   0.802  0.854555\n",
      "8      8   0.909369   0.893  0.901110\n",
      "9      9   0.885230   0.887  0.886114\n",
      "\n",
      "Accuracy  :  0.8159999847412109\n"
     ]
    }
   ],
   "source": [
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/vanilla_CNN_CIFAR10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To Retrieve\n",
    "Modelx = vanilla_CNN(1024,10)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/vanilla_CNN_CIFAR10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
