{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain dataset\n",
    "trainset = datasets.FashionMNIST('./../0. Data/',\n",
    "                               download = True,\n",
    "                               train = True,\n",
    "                               transform = T.Compose([\n",
    "                                   T.ToTensor()\n",
    "                               ]))\n",
    "\n",
    "testset = datasets.FashionMNIST('./../0. Data/',\n",
    "                              download = True,\n",
    "                              train = False,\n",
    "                              transform = T.Compose([\n",
    "                                  T.ToTensor()\n",
    "                              ]))\n",
    "\n",
    "#split training data further to training and validation data\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "#Put in dataloader\n",
    "train_loader_full = DataLoader(trainset,\n",
    "                           batch_size=32,\n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                       batch_size = 32,\n",
    "                       shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                        batch_size = 32,\n",
    "                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#check for GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a MLP class defining our neural network\n",
    "class vanilla_CNN(nn.Module):\n",
    "    def __init__(self, input_len, output_len):\n",
    "        super(vanilla_CNN, self).__init__()\n",
    "        \n",
    "        #Convvolutional and max pool layers\n",
    "        self.conv2d_32 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_1 = nn.BatchNorm2d(32)\n",
    "        self.conv2d_64 =  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_2 = nn.BatchNorm2d(64)\n",
    "        self.maxp2d_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2d_128 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_3 = nn.BatchNorm2d(128)\n",
    "        self.maxp2d_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2d_256 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), padding=1)\n",
    "        self.batch2d_4 = nn.BatchNorm2d(256)\n",
    "        self.maxp2d_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #three fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=256*3*3, out_features=512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,28,28)\n",
    "        #input_size = 1x28x28, output_size = 32x28x28\n",
    "        x = F.relu(self.conv2d_32(x))\n",
    "        #input_size = 32x28x28, output_size = 32x28x28\n",
    "        x = self.batch2d_1(x)\n",
    "        #input_size = 32x28x28, output_size = 64x28x28\n",
    "        x = F.relu(self.conv2d_64(x))\n",
    "        #input_size = 64x28x28, output_size = 64x28x28\n",
    "        x = self.batch2d_2(x)\n",
    "        #input_size = 64x28x28, output_size = 64x15x15\n",
    "        x = F.relu(self.maxp2d_1(x))\n",
    "        #input_size = 64x15x15, output_size = 128x15x15\n",
    "        x = F.relu(self.conv2d_128(x))\n",
    "        #input_size = 128x15x15, output_size = 128x15x15\n",
    "        x = self.batch2d_3(x)\n",
    "        #input_size = 128x15x15, output_size = 128x7x7\n",
    "        x = F.relu(self.maxp2d_2(x))\n",
    "        #input_size = 128x7x7, output_size = 256x7x7\n",
    "        x = F.relu(self.conv2d_256(x))\n",
    "        #input_size = 256x7x7, output_size = 256x7x7\n",
    "        x = self.batch2d_4(x)\n",
    "        #input_size = 256x7x7, output_size = 256x3x3\n",
    "        x = F.relu(self.maxp2d_3(x))\n",
    "\n",
    "        #convert image to a one dimentional tensor before feeding to neural network\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, validate, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = train_loader_full\n",
    "        \n",
    "        if(validate):\n",
    "            loader = train_loader\n",
    "        \n",
    "        cnf_tr = torch.zeros(10,10)\n",
    "        cnf_val = torch.zeros(10,10)\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "\n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradiuents to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].to(device)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #calculate output by argmax\n",
    "                output = torch.argmax(output, 1)\n",
    "                #update entries in confusion matrix\n",
    "                for i in range(output.shape[0]):\n",
    "                    cnf_tr[output[i],sample[1][i]] +=1\n",
    "            \n",
    "        if(validate):\n",
    "            #Evaluate on validation data\n",
    "            with torch.no_grad():\n",
    "                #set model to evaluation mode\n",
    "                Model.eval()\n",
    "                #evaluate on tvaidation data\n",
    "                for i, sample in enumerate(val_loader):\n",
    "                    output = Model(sample[0].to(device))\n",
    "                    loss = loss_function(output, sample[1].to(device))\n",
    "                    Val_Loss.append(loss.item())\n",
    "                    #calculate output by argmax\n",
    "                    output = torch.argmax(output, 1)\n",
    "                    #update entries in confusion matrix\n",
    "                    for i in range(output.shape[0]):\n",
    "                        cnf_val[output[i],sample[1][i]] +=1\n",
    "                   \n",
    "        actual_count = torch.sum(cnf_tr, dim=0)\n",
    "        correct_pred = torch.tensor([cnf_tr[i,i] for i in range(10)])\n",
    "        A_tr = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        if(validate):\n",
    "            actual_count = torch.sum(cnf_val, dim=0)\n",
    "            correct_pred = torch.tensor([cnf_val[i,i] for i in range(10)])\n",
    "            A_val = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        #print losses in every epoch\n",
    "        if(validate):\n",
    "            print('epoch : ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Val_acc : ', np.round(A_val,4),  \n",
    "                  '; Train_loss  ',np.round(np.mean(Train_Loss),4),  '; Val_loss  ',np.round(np.mean(Val_Loss),4))\n",
    "        else:\n",
    "            print('epoch = ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function top evaluate model using performace metrices\n",
    "def evaluate(cnf):\n",
    "    actual_count = torch.sum(cnf, dim=0)\n",
    "    predicted_count = torch.sum(cnf, dim=1)\n",
    "    correct_pred = torch.tensor([cnf[i,i] for i in range(10)])\n",
    "    #Precision\n",
    "    precision = correct_pred/predicted_count\n",
    "    #Recall\n",
    "    recall = correct_pred/actual_count\n",
    "    #F1-Score\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    #Accuracy\n",
    "    Accuracy = torch.sum(correct_pred)/torch.sum(actual_count)\n",
    "    print('\\n',pd.DataFrame({'Class':[i for i in range(10)],\n",
    "                 'Precision' : precision,\n",
    "                 'Recall' : recall,\n",
    "                 'F1_Score': f1_score}))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy  : ', Accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model):\n",
    "    Loss = []\n",
    "    #confusion matrix\n",
    "    cnf = torch.zeros(10,10)\n",
    "\n",
    "    #evaluate on test data\n",
    "    with torch.no_grad():\n",
    "        #set model to evaluation mode\n",
    "        Model.eval()\n",
    "        #evaluate on test data\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            output = Model(sample[0].to(device))\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            Loss.append(loss.item())\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #update entries in confusion matrix\n",
    "            for i in range(output.shape[0]):\n",
    "                cnf[output[i],sample[1][i]] +=1\n",
    "\n",
    "        #print test loss\n",
    "        print('Test loss : ', np.mean(Loss))\n",
    "\n",
    "    #print evaluation summary\n",
    "    evaluate(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.8528 ; Val_acc :  0.8942 ; Train_loss   0.4134 ; Val_loss   0.6838\n",
      "epoch :  1 ; Train_acc :  0.9002 ; Val_acc :  0.8992 ; Train_loss   0.2774 ; Val_loss   0.2952\n",
      "epoch :  2 ; Train_acc :  0.9156 ; Val_acc :  0.9152 ; Train_loss   0.2381 ; Val_loss   0.2425\n",
      "epoch :  3 ; Train_acc :  0.9249 ; Val_acc :  0.9244 ; Train_loss   0.2061 ; Val_loss   0.2122\n",
      "epoch :  4 ; Train_acc :  0.9355 ; Val_acc :  0.9278 ; Train_loss   0.1799 ; Val_loss   0.2013\n",
      "epoch :  5 ; Train_acc :  0.9422 ; Val_acc :  0.9266 ; Train_loss   0.1593 ; Val_loss   0.2217\n",
      "epoch :  6 ; Train_acc :  0.952 ; Val_acc :  0.9297 ; Train_loss   0.1354 ; Val_loss   0.2099\n",
      "epoch :  7 ; Train_acc :  0.9584 ; Val_acc :  0.9277 ; Train_loss   0.1177 ; Val_loss   0.2348\n",
      "epoch :  8 ; Train_acc :  0.9648 ; Val_acc :  0.9313 ; Train_loss   0.0978 ; Val_loss   0.2264\n",
      "epoch :  9 ; Train_acc :  0.9713 ; Val_acc :  0.9305 ; Train_loss   0.0821 ; Val_loss   0.2625\n",
      "epoch :  10 ; Train_acc :  0.9744 ; Val_acc :  0.9342 ; Train_loss   0.0715 ; Val_loss   0.2505\n",
      "epoch :  11 ; Train_acc :  0.9781 ; Val_acc :  0.9321 ; Train_loss   0.0637 ; Val_loss   0.2466\n",
      "epoch :  12 ; Train_acc :  0.98 ; Val_acc :  0.9328 ; Train_loss   0.0584 ; Val_loss   0.2556\n",
      "epoch :  13 ; Train_acc :  0.982 ; Val_acc :  0.9296 ; Train_loss   0.0503 ; Val_loss   0.2858\n",
      "epoch :  14 ; Train_acc :  0.9841 ; Val_acc :  0.9317 ; Train_loss   0.0447 ; Val_loss   0.2729\n",
      "epoch :  15 ; Train_acc :  0.9858 ; Val_acc :  0.9182 ; Train_loss   0.0406 ; Val_loss   0.3299\n",
      "epoch :  16 ; Train_acc :  0.9828 ; Val_acc :  0.9321 ; Train_loss   0.0491 ; Val_loss   0.3042\n",
      "epoch :  17 ; Train_acc :  0.9902 ; Val_acc :  0.9316 ; Train_loss   0.0294 ; Val_loss   0.3184\n",
      "epoch :  18 ; Train_acc :  0.9882 ; Val_acc :  0.9296 ; Train_loss   0.0332 ; Val_loss   0.3116\n",
      "epoch :  19 ; Train_acc :  0.989 ; Val_acc :  0.9294 ; Train_loss   0.0315 ; Val_loss   0.3341\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = vanilla_CNN(784,10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, validate=True, max_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.8594 ; Train_loss   0.3909\n",
      "epoch =  1 ; Train_acc :  0.9046 ; Train_loss   0.2649\n",
      "epoch =  2 ; Train_acc :  0.9188 ; Train_loss   0.2264\n",
      "epoch =  3 ; Train_acc :  0.9305 ; Train_loss   0.1956\n",
      "epoch =  4 ; Train_acc :  0.9383 ; Train_loss   0.1702\n",
      "epoch =  5 ; Train_acc :  0.9482 ; Train_loss   0.1459\n",
      "epoch =  6 ; Train_acc :  0.956 ; Train_loss   0.1216\n",
      "epoch =  7 ; Train_acc :  0.9631 ; Train_loss   0.1062\n",
      "epoch =  8 ; Train_acc :  0.9655 ; Train_loss   0.0957\n",
      "epoch =  9 ; Train_acc :  0.9721 ; Train_loss   0.0792\n",
      "epoch =  10 ; Train_acc :  0.9751 ; Train_loss   0.0692\n",
      "epoch =  11 ; Train_acc :  0.9779 ; Train_loss   0.0618\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 12 epochs on full training set\n",
    "#Create Model\n",
    "Model = vanilla_CNN(784,10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, validate=False, max_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.26729777438667257\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.873621   0.871  0.872308\n",
      "1      1   0.997972   0.984  0.990937\n",
      "2      2   0.879692   0.914  0.896518\n",
      "3      3   0.904986   0.962  0.932622\n",
      "4      4   0.891109   0.892  0.891554\n",
      "5      5   0.985972   0.984  0.984985\n",
      "6      6   0.836245   0.766  0.799582\n",
      "7      7   0.966469   0.980  0.973188\n",
      "8      8   0.991968   0.988  0.989980\n",
      "9      9   0.977778   0.968  0.972864\n",
      "\n",
      "Accuracy  :  0.930899977684021\n"
     ]
    }
   ],
   "source": [
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/vanilla_CNN_FMNIST.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To Retrieve\n",
    "Modelx = vanilla_CNN(784,10).to(device)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/vanilla_CNN_FMNIST.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
