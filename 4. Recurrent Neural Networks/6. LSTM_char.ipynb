{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for char level seq prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    \n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        #read from csv file:\n",
    "        data = []\n",
    "        with open(root_dir+txt_file, 'r') as file:\n",
    "            data.append(str(file.read().replace('\\n', '').replace('\\'', '').replace('\"','')))\n",
    "        #Preprocess data\n",
    "        data = list(data[0])\n",
    "        #make a dictionary of words\n",
    "        full_set = set([])\n",
    "        full_set = full_set.union(set(data)) \n",
    "        full_set = dict(enumerate(full_set))\n",
    "        self.full_set = {c: i for i, c in full_set.items()}\n",
    "        data = [self.full_set[i] for i in data]\n",
    "        self.data = np.array(data)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    #function to return length of data\n",
    "    def  __len__(self):\n",
    "        #we can take first 10 entries a s we will not be having last 10 features for them\n",
    "        return len(self.data)-10\n",
    "    \n",
    "    #function to get data\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx_label = idx+10\n",
    "        #for letter i, features are last 10 letter\n",
    "        idx_features = [idx_label-i for i in range (1,11)]\n",
    "       \n",
    "        #we assign next index as label for current sequence\n",
    "        sample = {'features' : [self.data[idx_features[i]] for i in range(10)],\n",
    "                     'label' : self.data[idx_label] \n",
    "                 }\n",
    "        \n",
    "        #apply transformation\n",
    "        if(self.transform):\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ToTensor class\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        feature = sample['features']\n",
    "        #one hot encoding of features\n",
    "        arr = np.zeros((len(feature),72))\n",
    "        for i in range(len(feature)):\n",
    "            arr[i,feature[i]] = 1\n",
    "            \n",
    "        label = sample['label']\n",
    "        \n",
    "        return [ torch.tensor(arr), torch.tensor(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain data\n",
    "data = MyData(txt_file='text_data.txt', \n",
    "              root_dir='./../0. Data/',\n",
    "              transform = T.Compose([\n",
    "                ToTensor()\n",
    "                ]))\n",
    "\n",
    "#we do not split data to train and test as it is a generative model with no fix target for any sequence\n",
    "data_loader = DataLoader(data,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 37, 56, 65, 37, 44, 45, 43, 58, 43, 43, 47, 32, 50, 37, 23, 38, 38,\n",
       "        11, 59, 32, 44, 38, 23, 43, 45, 34, 58, 38, 50, 22, 38])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vanilla LSTM class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        #save variables to use in other functions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        #define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        #convert output to desired output dimension(readout layer)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #call LSTM layer\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        #We will use only last output\n",
    "        out = self.fc(out[:,-1,:].view(x.shape[0],self.hidden_size))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = data_loader\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "            \n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].float().to(device).view(len(sample[0]),10,72)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        #print losses in every epoch\n",
    "        print('epoch = ', epoch,'; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model, seq, l):\n",
    "    \n",
    "    # save sequence to output\n",
    "    out = [i for i in seq]\n",
    "    \n",
    "    #define dictionaries to convert\n",
    "    char2int = data.full_set\n",
    "    int2char = {i : c for c , i in char2int.items()}\n",
    "    \n",
    "    seq = [char2int[i] for i in seq]\n",
    "    features = torch.zeros(10,72)\n",
    "    #one hot encoding\n",
    "    for i in range(10):\n",
    "        features[i,seq[i]] = 1\n",
    "    \n",
    "    #we run this for l iteration, in each iteration, we get letter at position l+i\n",
    "    with torch.no_grad():\n",
    "        for i in range(l):\n",
    "            #set model to evaluation mode\n",
    "            Model.eval()\n",
    "            output = Model(features.to(device).view(1,10,72))\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #append  word to output\n",
    "            out.append(int2char[output.item()])\n",
    "            features = features[1:]\n",
    "            temp = torch.zeros(1,72)\n",
    "            temp[0,output] = 1\n",
    "            features = torch.cat((features,temp), dim=0)\n",
    "            \n",
    "        print(''.join([i for i in out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_loss   3.0489\n",
      "epoch =  1 ; Train_loss   2.8395\n",
      "epoch =  2 ; Train_loss   2.5653\n",
      "epoch =  3 ; Train_loss   2.4403\n",
      "epoch =  4 ; Train_loss   2.329\n",
      "epoch =  5 ; Train_loss   2.2538\n",
      "epoch =  6 ; Train_loss   2.1983\n",
      "epoch =  7 ; Train_loss   2.146\n",
      "epoch =  8 ; Train_loss   2.1002\n",
      "epoch =  9 ; Train_loss   2.0482\n",
      "epoch =  10 ; Train_loss   1.9998\n",
      "epoch =  11 ; Train_loss   1.9582\n",
      "epoch =  12 ; Train_loss   1.9185\n",
      "epoch =  13 ; Train_loss   1.8868\n",
      "epoch =  14 ; Train_loss   1.8546\n",
      "epoch =  15 ; Train_loss   1.8276\n",
      "epoch =  16 ; Train_loss   1.801\n",
      "epoch =  17 ; Train_loss   1.7766\n",
      "epoch =  18 ; Train_loss   1.7516\n",
      "epoch =  19 ; Train_loss   1.7313\n",
      "epoch =  20 ; Train_loss   1.7081\n",
      "epoch =  21 ; Train_loss   1.6893\n",
      "epoch =  22 ; Train_loss   1.6698\n",
      "epoch =  23 ; Train_loss   1.6522\n",
      "epoch =  24 ; Train_loss   1.6346\n",
      "epoch =  25 ; Train_loss   1.618\n",
      "epoch =  26 ; Train_loss   1.5994\n",
      "epoch =  27 ; Train_loss   1.5866\n",
      "epoch =  28 ; Train_loss   1.5672\n",
      "epoch =  29 ; Train_loss   1.5539\n",
      "epoch =  30 ; Train_loss   1.5382\n",
      "epoch =  31 ; Train_loss   1.5267\n",
      "epoch =  32 ; Train_loss   1.5119\n",
      "epoch =  33 ; Train_loss   1.4987\n",
      "epoch =  34 ; Train_loss   1.4833\n",
      "epoch =  35 ; Train_loss   1.4728\n",
      "epoch =  36 ; Train_loss   1.4595\n",
      "epoch =  37 ; Train_loss   1.4497\n",
      "epoch =  38 ; Train_loss   1.4374\n",
      "epoch =  39 ; Train_loss   1.4252\n",
      "epoch =  40 ; Train_loss   1.4145\n",
      "epoch =  41 ; Train_loss   1.4052\n",
      "epoch =  42 ; Train_loss   1.3909\n",
      "epoch =  43 ; Train_loss   1.3808\n",
      "epoch =  44 ; Train_loss   1.3748\n",
      "epoch =  45 ; Train_loss   1.3622\n",
      "epoch =  46 ; Train_loss   1.3518\n",
      "epoch =  47 ; Train_loss   1.3427\n",
      "epoch =  48 ; Train_loss   1.3319\n",
      "epoch =  49 ; Train_loss   1.3228\n",
      "epoch =  50 ; Train_loss   1.3161\n",
      "epoch =  51 ; Train_loss   1.3072\n",
      "epoch =  52 ; Train_loss   1.296\n",
      "epoch =  53 ; Train_loss   1.2863\n",
      "epoch =  54 ; Train_loss   1.2775\n",
      "epoch =  55 ; Train_loss   1.2684\n",
      "epoch =  56 ; Train_loss   1.2602\n",
      "epoch =  57 ; Train_loss   1.252\n",
      "epoch =  58 ; Train_loss   1.2412\n",
      "epoch =  59 ; Train_loss   1.2335\n",
      "epoch =  60 ; Train_loss   1.2244\n",
      "epoch =  61 ; Train_loss   1.2177\n",
      "epoch =  62 ; Train_loss   1.2088\n",
      "epoch =  63 ; Train_loss   1.1996\n",
      "epoch =  64 ; Train_loss   1.1936\n",
      "epoch =  65 ; Train_loss   1.1828\n",
      "epoch =  66 ; Train_loss   1.1747\n",
      "epoch =  67 ; Train_loss   1.1673\n",
      "epoch =  68 ; Train_loss   1.1576\n",
      "epoch =  69 ; Train_loss   1.1505\n",
      "epoch =  70 ; Train_loss   1.1446\n",
      "epoch =  71 ; Train_loss   1.1357\n",
      "epoch =  72 ; Train_loss   1.1285\n",
      "epoch =  73 ; Train_loss   1.1192\n",
      "epoch =  74 ; Train_loss   1.1118\n",
      "epoch =  75 ; Train_loss   1.1055\n",
      "epoch =  76 ; Train_loss   1.0998\n",
      "epoch =  77 ; Train_loss   1.0878\n",
      "epoch =  78 ; Train_loss   1.08\n",
      "epoch =  79 ; Train_loss   1.0758\n",
      "epoch =  80 ; Train_loss   1.0678\n",
      "epoch =  81 ; Train_loss   1.0594\n",
      "epoch =  82 ; Train_loss   1.0525\n",
      "epoch =  83 ; Train_loss   1.0459\n",
      "epoch =  84 ; Train_loss   1.0392\n",
      "epoch =  85 ; Train_loss   1.0324\n",
      "epoch =  86 ; Train_loss   1.0272\n",
      "epoch =  87 ; Train_loss   1.0198\n",
      "epoch =  88 ; Train_loss   1.0135\n",
      "epoch =  89 ; Train_loss   1.0049\n",
      "epoch =  90 ; Train_loss   1.0002\n",
      "epoch =  91 ; Train_loss   0.994\n",
      "epoch =  92 ; Train_loss   0.9862\n",
      "epoch =  93 ; Train_loss   0.9829\n",
      "epoch =  94 ; Train_loss   0.9724\n",
      "epoch =  95 ; Train_loss   0.9694\n",
      "epoch =  96 ; Train_loss   0.9611\n",
      "epoch =  97 ; Train_loss   0.9564\n",
      "epoch =  98 ; Train_loss   0.951\n",
      "epoch =  99 ; Train_loss   0.947\n",
      "epoch =  100 ; Train_loss   0.9388\n",
      "epoch =  101 ; Train_loss   0.9329\n",
      "epoch =  102 ; Train_loss   0.9275\n",
      "epoch =  103 ; Train_loss   0.92\n",
      "epoch =  104 ; Train_loss   0.9178\n",
      "epoch =  105 ; Train_loss   0.9076\n",
      "epoch =  106 ; Train_loss   0.9008\n",
      "epoch =  107 ; Train_loss   0.8991\n",
      "epoch =  108 ; Train_loss   0.8933\n",
      "epoch =  109 ; Train_loss   0.8841\n",
      "epoch =  110 ; Train_loss   0.8818\n",
      "epoch =  111 ; Train_loss   0.8767\n",
      "epoch =  112 ; Train_loss   0.8707\n",
      "epoch =  113 ; Train_loss   0.8677\n",
      "epoch =  114 ; Train_loss   0.8604\n",
      "epoch =  115 ; Train_loss   0.8561\n",
      "epoch =  116 ; Train_loss   0.8515\n",
      "epoch =  117 ; Train_loss   0.8469\n",
      "epoch =  118 ; Train_loss   0.8421\n",
      "epoch =  119 ; Train_loss   0.8417\n",
      "epoch =  120 ; Train_loss   0.8311\n",
      "epoch =  121 ; Train_loss   0.8269\n",
      "epoch =  122 ; Train_loss   0.8274\n",
      "epoch =  123 ; Train_loss   0.823\n",
      "epoch =  124 ; Train_loss   0.8145\n",
      "epoch =  125 ; Train_loss   0.8114\n",
      "epoch =  126 ; Train_loss   0.8117\n",
      "epoch =  127 ; Train_loss   0.801\n",
      "epoch =  128 ; Train_loss   0.7987\n",
      "epoch =  129 ; Train_loss   0.7916\n",
      "epoch =  130 ; Train_loss   0.7956\n",
      "epoch =  131 ; Train_loss   0.7856\n",
      "epoch =  132 ; Train_loss   0.7812\n",
      "epoch =  133 ; Train_loss   0.7791\n",
      "epoch =  134 ; Train_loss   0.7758\n",
      "epoch =  135 ; Train_loss   0.7704\n",
      "epoch =  136 ; Train_loss   0.774\n",
      "epoch =  137 ; Train_loss   0.7605\n",
      "epoch =  138 ; Train_loss   0.7592\n",
      "epoch =  139 ; Train_loss   0.7508\n",
      "epoch =  140 ; Train_loss   0.7531\n",
      "epoch =  141 ; Train_loss   0.7503\n",
      "epoch =  142 ; Train_loss   0.749\n",
      "epoch =  143 ; Train_loss   0.736\n",
      "epoch =  144 ; Train_loss   0.7397\n",
      "epoch =  145 ; Train_loss   0.738\n",
      "epoch =  146 ; Train_loss   0.7267\n",
      "epoch =  147 ; Train_loss   0.7284\n",
      "epoch =  148 ; Train_loss   0.722\n",
      "epoch =  149 ; Train_loss   0.7305\n",
      "epoch =  150 ; Train_loss   0.7183\n",
      "epoch =  151 ; Train_loss   0.7116\n",
      "epoch =  152 ; Train_loss   0.7123\n",
      "epoch =  153 ; Train_loss   0.7048\n",
      "epoch =  154 ; Train_loss   0.7076\n",
      "epoch =  155 ; Train_loss   0.7027\n",
      "epoch =  156 ; Train_loss   0.695\n",
      "epoch =  157 ; Train_loss   0.6933\n",
      "epoch =  158 ; Train_loss   0.6925\n",
      "epoch =  159 ; Train_loss   0.6917\n",
      "epoch =  160 ; Train_loss   0.6857\n",
      "epoch =  161 ; Train_loss   0.6881\n",
      "epoch =  162 ; Train_loss   0.6812\n",
      "epoch =  163 ; Train_loss   0.6763\n",
      "epoch =  164 ; Train_loss   0.6733\n",
      "epoch =  165 ; Train_loss   0.6735\n",
      "epoch =  166 ; Train_loss   0.6729\n",
      "epoch =  167 ; Train_loss   0.6646\n",
      "epoch =  168 ; Train_loss   0.6656\n",
      "epoch =  169 ; Train_loss   0.6591\n",
      "epoch =  170 ; Train_loss   0.6626\n",
      "epoch =  171 ; Train_loss   0.6577\n",
      "epoch =  172 ; Train_loss   0.6502\n",
      "epoch =  173 ; Train_loss   0.6585\n",
      "epoch =  174 ; Train_loss   0.6429\n",
      "epoch =  175 ; Train_loss   0.6519\n",
      "epoch =  176 ; Train_loss   0.6373\n",
      "epoch =  177 ; Train_loss   0.6428\n",
      "epoch =  178 ; Train_loss   0.6449\n",
      "epoch =  179 ; Train_loss   0.6339\n",
      "epoch =  180 ; Train_loss   0.634\n",
      "epoch =  181 ; Train_loss   0.6376\n",
      "epoch =  182 ; Train_loss   0.6319\n",
      "epoch =  183 ; Train_loss   0.6298\n",
      "epoch =  184 ; Train_loss   0.6155\n",
      "epoch =  185 ; Train_loss   0.6275\n",
      "epoch =  186 ; Train_loss   0.62\n",
      "epoch =  187 ; Train_loss   0.6233\n",
      "epoch =  188 ; Train_loss   0.6113\n",
      "epoch =  189 ; Train_loss   0.6128\n",
      "epoch =  190 ; Train_loss   0.609\n",
      "epoch =  191 ; Train_loss   0.6127\n",
      "epoch =  192 ; Train_loss   0.6131\n",
      "epoch =  193 ; Train_loss   0.6068\n",
      "epoch =  194 ; Train_loss   0.6072\n",
      "epoch =  195 ; Train_loss   0.6006\n",
      "epoch =  196 ; Train_loss   0.6022\n",
      "epoch =  197 ; Train_loss   0.6025\n",
      "epoch =  198 ; Train_loss   0.5993\n",
      "epoch =  199 ; Train_loss   0.5908\n",
      "epoch =  200 ; Train_loss   0.5951\n",
      "epoch =  201 ; Train_loss   0.5978\n",
      "epoch =  202 ; Train_loss   0.5901\n",
      "epoch =  203 ; Train_loss   0.5857\n",
      "epoch =  204 ; Train_loss   0.5918\n",
      "epoch =  205 ; Train_loss   0.5893\n",
      "epoch =  206 ; Train_loss   0.5843\n",
      "epoch =  207 ; Train_loss   0.5958\n",
      "epoch =  208 ; Train_loss   0.5848\n",
      "epoch =  209 ; Train_loss   0.5841\n",
      "epoch =  210 ; Train_loss   0.5849\n",
      "epoch =  211 ; Train_loss   0.5764\n",
      "epoch =  212 ; Train_loss   0.5737\n",
      "epoch =  213 ; Train_loss   0.5755\n",
      "epoch =  214 ; Train_loss   0.5725\n",
      "epoch =  215 ; Train_loss   0.5749\n",
      "epoch =  216 ; Train_loss   0.5693\n",
      "epoch =  217 ; Train_loss   0.561\n",
      "epoch =  218 ; Train_loss   0.5706\n",
      "epoch =  219 ; Train_loss   0.5697\n",
      "epoch =  220 ; Train_loss   0.5657\n",
      "epoch =  221 ; Train_loss   0.5669\n",
      "epoch =  222 ; Train_loss   0.5678\n",
      "epoch =  223 ; Train_loss   0.5617\n",
      "epoch =  224 ; Train_loss   0.5644\n",
      "epoch =  225 ; Train_loss   0.5555\n",
      "epoch =  226 ; Train_loss   0.5471\n",
      "epoch =  227 ; Train_loss   0.5597\n",
      "epoch =  228 ; Train_loss   0.5578\n",
      "epoch =  229 ; Train_loss   0.548\n",
      "epoch =  230 ; Train_loss   0.5451\n",
      "epoch =  231 ; Train_loss   0.5534\n",
      "epoch =  232 ; Train_loss   0.5383\n",
      "epoch =  233 ; Train_loss   0.5522\n",
      "epoch =  234 ; Train_loss   0.5443\n",
      "epoch =  235 ; Train_loss   0.5529\n",
      "epoch =  236 ; Train_loss   0.5351\n",
      "epoch =  237 ; Train_loss   0.5408\n",
      "epoch =  238 ; Train_loss   0.5522\n",
      "epoch =  239 ; Train_loss   0.5396\n",
      "epoch =  240 ; Train_loss   0.5397\n",
      "epoch =  241 ; Train_loss   0.5474\n",
      "epoch =  242 ; Train_loss   0.5424\n",
      "epoch =  243 ; Train_loss   0.5346\n",
      "epoch =  244 ; Train_loss   0.5324\n",
      "epoch =  245 ; Train_loss   0.5382\n",
      "epoch =  246 ; Train_loss   0.5358\n",
      "epoch =  247 ; Train_loss   0.5286\n",
      "epoch =  248 ; Train_loss   0.532\n",
      "epoch =  249 ; Train_loss   0.5313\n",
      "epoch =  250 ; Train_loss   0.527\n",
      "epoch =  251 ; Train_loss   0.5252\n",
      "epoch =  252 ; Train_loss   0.527\n",
      "epoch =  253 ; Train_loss   0.5311\n",
      "epoch =  254 ; Train_loss   0.5189\n",
      "epoch =  255 ; Train_loss   0.5194\n",
      "epoch =  256 ; Train_loss   0.5221\n",
      "epoch =  257 ; Train_loss   0.5166\n",
      "epoch =  258 ; Train_loss   0.5268\n",
      "epoch =  259 ; Train_loss   0.5223\n",
      "epoch =  260 ; Train_loss   0.5147\n",
      "epoch =  261 ; Train_loss   0.5243\n",
      "epoch =  262 ; Train_loss   0.5133\n",
      "epoch =  263 ; Train_loss   0.5257\n",
      "epoch =  264 ; Train_loss   0.509\n",
      "epoch =  265 ; Train_loss   0.5317\n",
      "epoch =  266 ; Train_loss   0.5212\n",
      "epoch =  267 ; Train_loss   0.5149\n",
      "epoch =  268 ; Train_loss   0.5198\n",
      "epoch =  269 ; Train_loss   0.5146\n",
      "epoch =  270 ; Train_loss   0.5179\n",
      "epoch =  271 ; Train_loss   0.5149\n",
      "epoch =  272 ; Train_loss   0.5122\n",
      "epoch =  273 ; Train_loss   0.5147\n",
      "epoch =  274 ; Train_loss   0.5107\n",
      "epoch =  275 ; Train_loss   0.5135\n",
      "epoch =  276 ; Train_loss   0.5193\n",
      "epoch =  277 ; Train_loss   0.5055\n",
      "epoch =  278 ; Train_loss   0.5116\n",
      "epoch =  279 ; Train_loss   0.5079\n",
      "epoch =  280 ; Train_loss   0.5133\n",
      "epoch =  281 ; Train_loss   0.5056\n",
      "epoch =  282 ; Train_loss   0.505\n",
      "epoch =  283 ; Train_loss   0.5102\n",
      "epoch =  284 ; Train_loss   0.505\n",
      "epoch =  285 ; Train_loss   0.5102\n",
      "epoch =  286 ; Train_loss   0.5001\n",
      "epoch =  287 ; Train_loss   0.4977\n",
      "epoch =  288 ; Train_loss   0.5177\n",
      "epoch =  289 ; Train_loss   0.4904\n",
      "epoch =  290 ; Train_loss   0.493\n",
      "epoch =  291 ; Train_loss   0.502\n",
      "epoch =  292 ; Train_loss   0.4967\n",
      "epoch =  293 ; Train_loss   0.5011\n",
      "epoch =  294 ; Train_loss   0.4997\n",
      "epoch =  295 ; Train_loss   0.4982\n",
      "epoch =  296 ; Train_loss   0.5094\n",
      "epoch =  297 ; Train_loss   0.4954\n",
      "epoch =  298 ; Train_loss   0.4928\n",
      "epoch =  299 ; Train_loss   0.4837\n",
      "epoch =  300 ; Train_loss   0.5051\n",
      "epoch =  301 ; Train_loss   0.489\n",
      "epoch =  302 ; Train_loss   0.4872\n",
      "epoch =  303 ; Train_loss   0.4914\n",
      "epoch =  304 ; Train_loss   0.4903\n",
      "epoch =  305 ; Train_loss   0.4877\n",
      "epoch =  306 ; Train_loss   0.4831\n",
      "epoch =  307 ; Train_loss   0.489\n",
      "epoch =  308 ; Train_loss   0.4881\n",
      "epoch =  309 ; Train_loss   0.482\n",
      "epoch =  310 ; Train_loss   0.4922\n",
      "epoch =  311 ; Train_loss   0.4843\n",
      "epoch =  312 ; Train_loss   0.4824\n",
      "epoch =  313 ; Train_loss   0.4885\n",
      "epoch =  314 ; Train_loss   0.4848\n",
      "epoch =  315 ; Train_loss   0.4897\n",
      "epoch =  316 ; Train_loss   0.4829\n",
      "epoch =  317 ; Train_loss   0.4827\n",
      "epoch =  318 ; Train_loss   0.4857\n",
      "epoch =  319 ; Train_loss   0.4757\n",
      "epoch =  320 ; Train_loss   0.4765\n",
      "epoch =  321 ; Train_loss   0.4746\n",
      "epoch =  322 ; Train_loss   0.4695\n",
      "epoch =  323 ; Train_loss   0.485\n",
      "epoch =  324 ; Train_loss   0.4794\n",
      "epoch =  325 ; Train_loss   0.4837\n",
      "epoch =  326 ; Train_loss   0.4626\n",
      "epoch =  327 ; Train_loss   0.4813\n",
      "epoch =  328 ; Train_loss   0.4713\n",
      "epoch =  329 ; Train_loss   0.4672\n",
      "epoch =  330 ; Train_loss   0.4859\n",
      "epoch =  331 ; Train_loss   0.4709\n",
      "epoch =  332 ; Train_loss   0.478\n",
      "epoch =  333 ; Train_loss   0.4837\n",
      "epoch =  334 ; Train_loss   0.469\n",
      "epoch =  335 ; Train_loss   0.4769\n",
      "epoch =  336 ; Train_loss   0.481\n",
      "epoch =  337 ; Train_loss   0.4751\n",
      "epoch =  338 ; Train_loss   0.4766\n",
      "epoch =  339 ; Train_loss   0.4752\n",
      "epoch =  340 ; Train_loss   0.4667\n",
      "epoch =  341 ; Train_loss   0.4676\n",
      "epoch =  342 ; Train_loss   0.4714\n",
      "epoch =  343 ; Train_loss   0.4623\n",
      "epoch =  344 ; Train_loss   0.4757\n",
      "epoch =  345 ; Train_loss   0.4674\n",
      "epoch =  346 ; Train_loss   0.4644\n",
      "epoch =  347 ; Train_loss   0.4665\n",
      "epoch =  348 ; Train_loss   0.4636\n",
      "epoch =  349 ; Train_loss   0.4649\n",
      "epoch =  350 ; Train_loss   0.481\n",
      "epoch =  351 ; Train_loss   0.468\n",
      "epoch =  352 ; Train_loss   0.466\n",
      "epoch =  353 ; Train_loss   0.4742\n",
      "epoch =  354 ; Train_loss   0.4664\n",
      "epoch =  355 ; Train_loss   0.4715\n",
      "epoch =  356 ; Train_loss   0.4644\n",
      "epoch =  357 ; Train_loss   0.4654\n",
      "epoch =  358 ; Train_loss   0.467\n",
      "epoch =  359 ; Train_loss   0.4633\n",
      "epoch =  360 ; Train_loss   0.4679\n",
      "epoch =  361 ; Train_loss   0.4599\n",
      "epoch =  362 ; Train_loss   0.4629\n",
      "epoch =  363 ; Train_loss   0.4653\n",
      "epoch =  364 ; Train_loss   0.4622\n",
      "epoch =  365 ; Train_loss   0.4611\n",
      "epoch =  366 ; Train_loss   0.4664\n",
      "epoch =  367 ; Train_loss   0.4472\n",
      "epoch =  368 ; Train_loss   0.4648\n",
      "epoch =  369 ; Train_loss   0.465\n",
      "epoch =  370 ; Train_loss   0.4519\n",
      "epoch =  371 ; Train_loss   0.4672\n",
      "epoch =  372 ; Train_loss   0.4471\n",
      "epoch =  373 ; Train_loss   0.4649\n",
      "epoch =  374 ; Train_loss   0.4579\n",
      "epoch =  375 ; Train_loss   0.4498\n",
      "epoch =  376 ; Train_loss   0.4413\n",
      "epoch =  377 ; Train_loss   0.4641\n",
      "epoch =  378 ; Train_loss   0.4638\n",
      "epoch =  379 ; Train_loss   0.4476\n",
      "epoch =  380 ; Train_loss   0.4542\n",
      "epoch =  381 ; Train_loss   0.4588\n",
      "epoch =  382 ; Train_loss   0.4685\n",
      "epoch =  383 ; Train_loss   0.4382\n",
      "epoch =  384 ; Train_loss   0.4539\n",
      "epoch =  385 ; Train_loss   0.4388\n",
      "epoch =  386 ; Train_loss   0.448\n",
      "epoch =  387 ; Train_loss   0.4563\n",
      "epoch =  388 ; Train_loss   0.4397\n",
      "epoch =  389 ; Train_loss   0.4426\n",
      "epoch =  390 ; Train_loss   0.4466\n",
      "epoch =  391 ; Train_loss   0.4501\n",
      "epoch =  392 ; Train_loss   0.444\n",
      "epoch =  393 ; Train_loss   0.4772\n",
      "epoch =  394 ; Train_loss   0.4467\n",
      "epoch =  395 ; Train_loss   0.4461\n",
      "epoch =  396 ; Train_loss   0.4523\n",
      "epoch =  397 ; Train_loss   0.4513\n",
      "epoch =  398 ; Train_loss   0.4509\n",
      "epoch =  399 ; Train_loss   0.4636\n",
      "epoch =  400 ; Train_loss   0.4508\n",
      "epoch =  401 ; Train_loss   0.4432\n",
      "epoch =  402 ; Train_loss   0.4485\n",
      "epoch =  403 ; Train_loss   0.4493\n",
      "epoch =  404 ; Train_loss   0.446\n",
      "epoch =  405 ; Train_loss   0.4524\n",
      "epoch =  406 ; Train_loss   0.4404\n",
      "epoch =  407 ; Train_loss   0.4451\n",
      "epoch =  408 ; Train_loss   0.4398\n",
      "epoch =  409 ; Train_loss   0.4334\n",
      "epoch =  410 ; Train_loss   0.4487\n",
      "epoch =  411 ; Train_loss   0.4413\n",
      "epoch =  412 ; Train_loss   0.4359\n",
      "epoch =  413 ; Train_loss   0.4535\n",
      "epoch =  414 ; Train_loss   0.4502\n",
      "epoch =  415 ; Train_loss   0.4369\n",
      "epoch =  416 ; Train_loss   0.4623\n",
      "epoch =  417 ; Train_loss   0.4434\n",
      "epoch =  418 ; Train_loss   0.4602\n",
      "epoch =  419 ; Train_loss   0.4413\n",
      "epoch =  420 ; Train_loss   0.4386\n",
      "epoch =  421 ; Train_loss   0.4414\n",
      "epoch =  422 ; Train_loss   0.4473\n",
      "epoch =  423 ; Train_loss   0.4338\n",
      "epoch =  424 ; Train_loss   0.4515\n",
      "epoch =  425 ; Train_loss   0.438\n",
      "epoch =  426 ; Train_loss   0.4379\n",
      "epoch =  427 ; Train_loss   0.4486\n",
      "epoch =  428 ; Train_loss   0.4424\n",
      "epoch =  429 ; Train_loss   0.4506\n",
      "epoch =  430 ; Train_loss   0.4426\n",
      "epoch =  431 ; Train_loss   0.427\n",
      "epoch =  432 ; Train_loss   0.4436\n",
      "epoch =  433 ; Train_loss   0.4339\n",
      "epoch =  434 ; Train_loss   0.4394\n",
      "epoch =  435 ; Train_loss   0.4364\n",
      "epoch =  436 ; Train_loss   0.4408\n",
      "epoch =  437 ; Train_loss   0.4364\n",
      "epoch =  438 ; Train_loss   0.438\n",
      "epoch =  439 ; Train_loss   0.427\n",
      "epoch =  440 ; Train_loss   0.447\n",
      "epoch =  441 ; Train_loss   0.4401\n",
      "epoch =  442 ; Train_loss   0.4345\n",
      "epoch =  443 ; Train_loss   0.4274\n",
      "epoch =  444 ; Train_loss   0.4382\n",
      "epoch =  445 ; Train_loss   0.4447\n",
      "epoch =  446 ; Train_loss   0.4233\n",
      "epoch =  447 ; Train_loss   0.4368\n",
      "epoch =  448 ; Train_loss   0.4405\n",
      "epoch =  449 ; Train_loss   0.4539\n",
      "epoch =  450 ; Train_loss   0.4314\n",
      "epoch =  451 ; Train_loss   0.4399\n",
      "epoch =  452 ; Train_loss   0.4496\n",
      "epoch =  453 ; Train_loss   0.4282\n",
      "epoch =  454 ; Train_loss   0.4358\n",
      "epoch =  455 ; Train_loss   0.447\n",
      "epoch =  456 ; Train_loss   0.4291\n",
      "epoch =  457 ; Train_loss   0.4346\n",
      "epoch =  458 ; Train_loss   0.4314\n",
      "epoch =  459 ; Train_loss   0.4292\n",
      "epoch =  460 ; Train_loss   0.4295\n",
      "epoch =  461 ; Train_loss   0.4339\n",
      "epoch =  462 ; Train_loss   0.4315\n",
      "epoch =  463 ; Train_loss   0.4491\n",
      "epoch =  464 ; Train_loss   0.4388\n",
      "epoch =  465 ; Train_loss   0.4435\n",
      "epoch =  466 ; Train_loss   0.4348\n",
      "epoch =  467 ; Train_loss   0.4335\n",
      "epoch =  468 ; Train_loss   0.4372\n",
      "epoch =  469 ; Train_loss   0.4326\n",
      "epoch =  470 ; Train_loss   0.4279\n",
      "epoch =  471 ; Train_loss   0.4343\n",
      "epoch =  472 ; Train_loss   0.4288\n",
      "epoch =  473 ; Train_loss   0.4328\n",
      "epoch =  474 ; Train_loss   0.4264\n",
      "epoch =  475 ; Train_loss   0.427\n",
      "epoch =  476 ; Train_loss   0.4323\n",
      "epoch =  477 ; Train_loss   0.4279\n",
      "epoch =  478 ; Train_loss   0.4219\n",
      "epoch =  479 ; Train_loss   0.4277\n",
      "epoch =  480 ; Train_loss   0.4171\n",
      "epoch =  481 ; Train_loss   0.4392\n",
      "epoch =  482 ; Train_loss   0.4299\n",
      "epoch =  483 ; Train_loss   0.4247\n",
      "epoch =  484 ; Train_loss   0.4262\n",
      "epoch =  485 ; Train_loss   0.4273\n",
      "epoch =  486 ; Train_loss   0.4234\n",
      "epoch =  487 ; Train_loss   0.427\n",
      "epoch =  488 ; Train_loss   0.4287\n",
      "epoch =  489 ; Train_loss   0.4243\n",
      "epoch =  490 ; Train_loss   0.4306\n",
      "epoch =  491 ; Train_loss   0.4259\n",
      "epoch =  492 ; Train_loss   0.429\n",
      "epoch =  493 ; Train_loss   0.4382\n",
      "epoch =  494 ; Train_loss   0.4311\n",
      "epoch =  495 ; Train_loss   0.4177\n",
      "epoch =  496 ; Train_loss   0.4208\n",
      "epoch =  497 ; Train_loss   0.4301\n",
      "epoch =  498 ; Train_loss   0.4261\n",
      "epoch =  499 ; Train_loss   0.419\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = LSTM(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, max_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = ['I dont kno']\n",
    "seq2 = ['There was ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont kno oeweot  uelrds hmssnueeaoia hitdfgpn a ofearaesnir-rlslltggobey yya, dotot, pvw,  a nnae nucsat,csw\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model,list(seq1[0]),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was oi, xmeneedn ocbd,deortnee   sno  vnswoeenmLae, urr;elt  ynae weemo ecvwfd ouo neouoettftlsubrdeeu e\n"
     ]
    }
   ],
   "source": [
    "test(Model,list(seq2[0]),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprized by results, we have implemented a very basic model and language modelling is a very dificult task. It requires huge architectures to generate something meaningful. So, we move to NLP where we will learn how to represent natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/LSTM_char.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Retrieve\n",
    "Modelx = LSTM(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/LSTM_char.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
