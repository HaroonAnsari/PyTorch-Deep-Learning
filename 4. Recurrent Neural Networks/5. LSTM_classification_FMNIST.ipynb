{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for classification on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's download data using torchvision\n",
    "trainset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = True, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "testset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = False, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "#split training data to training and validation  data\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "#Convert data to dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size = 32,\n",
    "                                        shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size = 32, \n",
    "                                         shuffle = True)\n",
    "\n",
    "full_train_set  = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create  LSTM class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        #save variables to use in other functions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        #define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        #convert output to desired output dimension(readout layer)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #call LSTM layer\n",
    "        x = x.view(x.shape[0],28,28)\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        #We will use only last output\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, validate, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = full_train_set\n",
    "        \n",
    "        if(validate):\n",
    "            loader = train_loader\n",
    "        \n",
    "        cnf_tr = torch.zeros(10,10)\n",
    "        cnf_val = torch.zeros(10,10)\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "\n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradiuents to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].to(device)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #calculate output by argmax\n",
    "                output = torch.argmax(output, 1)\n",
    "                #update entries in confusion matrix\n",
    "                for i in range(output.shape[0]):\n",
    "                    cnf_tr[output[i],sample[1][i]] +=1\n",
    "            \n",
    "        if(validate):\n",
    "            #Evaluate on validation data\n",
    "            with torch.no_grad():\n",
    "                #set model to evaluation mode\n",
    "                Model.eval()\n",
    "                #evaluate on tvaidation data\n",
    "                for i, sample in enumerate(val_loader):\n",
    "                    output = Model(sample[0].to(device))\n",
    "                    loss = loss_function(output, sample[1].to(device))\n",
    "                    Val_Loss.append(loss.item())\n",
    "                    #calculate output by argmax\n",
    "                    output = torch.argmax(output, 1)\n",
    "                    #update entries in confusion matrix\n",
    "                    for i in range(output.shape[0]):\n",
    "                        cnf_val[output[i],sample[1][i]] +=1\n",
    "                   \n",
    "        actual_count = torch.sum(cnf_tr, dim=0)\n",
    "        correct_pred = torch.tensor([cnf_tr[i,i] for i in range(10)])\n",
    "        A_tr = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        if(validate):\n",
    "            actual_count = torch.sum(cnf_val, dim=0)\n",
    "            correct_pred = torch.tensor([cnf_val[i,i] for i in range(10)])\n",
    "            A_val = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        #print losses in every epoch\n",
    "        if(validate):\n",
    "            print('epoch : ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Val_acc : ', np.round(A_val,4),  \n",
    "                  '; Train_loss  ',np.round(np.mean(Train_Loss),4),  '; Val_loss  ',np.round(np.mean(Val_Loss),4))\n",
    "        else:\n",
    "            print('epoch = ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function top evaluate model using performace metrices\n",
    "def evaluate(cnf):\n",
    "    actual_count = torch.sum(cnf, dim=0)\n",
    "    predicted_count = torch.sum(cnf, dim=1)\n",
    "    correct_pred = torch.tensor([cnf[i,i] for i in range(10)])\n",
    "    #Precision\n",
    "    precision = correct_pred/predicted_count\n",
    "    #Recall\n",
    "    recall = correct_pred/actual_count\n",
    "    #F1-Score\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    #Accuracy\n",
    "    Accuracy = torch.sum(correct_pred)/torch.sum(actual_count)\n",
    "    print('\\n',pd.DataFrame({'Class':[i for i in range(10)],\n",
    "                 'Precision' : precision,\n",
    "                 'Recall' : recall,\n",
    "                 'F1_Score': f1_score}))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy  : ', Accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model):\n",
    "    Loss = []\n",
    "    #confusion matrix\n",
    "    cnf = torch.zeros(10,10)\n",
    "\n",
    "    #evaluate on test data\n",
    "    with torch.no_grad():\n",
    "        #set model to evaluation mode\n",
    "        Model.eval()\n",
    "        #evaluate on test data\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            output = Model(sample[0].to(device))\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            Loss.append(loss.item())\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #update entries in confusion matrix\n",
    "            for i in range(output.shape[0]):\n",
    "                cnf[output[i],sample[1][i]] +=1\n",
    "\n",
    "        #print test loss\n",
    "        print('Test loss : ', np.mean(Loss))\n",
    "\n",
    "    #print evaluation summary\n",
    "    evaluate(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.7342 ; Val_acc :  0.8049 ; Train_loss   0.7277 ; Val_loss   0.5454\n",
      "epoch :  1 ; Train_acc :  0.8192 ; Val_acc :  0.8351 ; Train_loss   0.49 ; Val_loss   0.461\n",
      "epoch :  2 ; Train_acc :  0.8421 ; Val_acc :  0.8557 ; Train_loss   0.428 ; Val_loss   0.3977\n",
      "epoch :  3 ; Train_acc :  0.855 ; Val_acc :  0.8688 ; Train_loss   0.387 ; Val_loss   0.3564\n",
      "epoch :  4 ; Train_acc :  0.8642 ; Val_acc :  0.8647 ; Train_loss   0.3609 ; Val_loss   0.3665\n",
      "epoch :  5 ; Train_acc :  0.8742 ; Val_acc :  0.8789 ; Train_loss   0.3367 ; Val_loss   0.338\n",
      "epoch :  6 ; Train_acc :  0.8784 ; Val_acc :  0.883 ; Train_loss   0.3217 ; Val_loss   0.3179\n",
      "epoch :  7 ; Train_acc :  0.8874 ; Val_acc :  0.8857 ; Train_loss   0.303 ; Val_loss   0.3127\n",
      "epoch :  8 ; Train_acc :  0.8927 ; Val_acc :  0.8904 ; Train_loss   0.2889 ; Val_loss   0.3067\n",
      "epoch :  9 ; Train_acc :  0.8957 ; Val_acc :  0.8873 ; Train_loss   0.2757 ; Val_loss   0.3098\n",
      "epoch :  10 ; Train_acc :  0.9019 ; Val_acc :  0.8962 ; Train_loss   0.2641 ; Val_loss   0.2885\n",
      "epoch :  11 ; Train_acc :  0.9056 ; Val_acc :  0.8961 ; Train_loss   0.2527 ; Val_loss   0.2807\n",
      "epoch :  12 ; Train_acc :  0.9095 ; Val_acc :  0.8935 ; Train_loss   0.242 ; Val_loss   0.2857\n",
      "epoch :  13 ; Train_acc :  0.9123 ; Val_acc :  0.8942 ; Train_loss   0.2357 ; Val_loss   0.2867\n",
      "epoch :  14 ; Train_acc :  0.9143 ; Val_acc :  0.8993 ; Train_loss   0.2313 ; Val_loss   0.2796\n",
      "epoch :  15 ; Train_acc :  0.9179 ; Val_acc :  0.9044 ; Train_loss   0.2178 ; Val_loss   0.2716\n",
      "epoch :  16 ; Train_acc :  0.9203 ; Val_acc :  0.8985 ; Train_loss   0.2124 ; Val_loss   0.2844\n",
      "epoch :  17 ; Train_acc :  0.9227 ; Val_acc :  0.901 ; Train_loss   0.2049 ; Val_loss   0.2773\n",
      "epoch :  18 ; Train_acc :  0.9251 ; Val_acc :  0.8989 ; Train_loss   0.1985 ; Val_loss   0.287\n",
      "epoch :  19 ; Train_acc :  0.9296 ; Val_acc :  0.9045 ; Train_loss   0.1887 ; Val_loss   0.2789\n",
      "epoch :  20 ; Train_acc :  0.9306 ; Val_acc :  0.9031 ; Train_loss   0.1849 ; Val_loss   0.2776\n",
      "epoch :  21 ; Train_acc :  0.9329 ; Val_acc :  0.9072 ; Train_loss   0.1795 ; Val_loss   0.2751\n",
      "epoch :  22 ; Train_acc :  0.936 ; Val_acc :  0.9048 ; Train_loss   0.1725 ; Val_loss   0.2748\n",
      "epoch :  23 ; Train_acc :  0.9388 ; Val_acc :  0.9033 ; Train_loss   0.1636 ; Val_loss   0.2893\n",
      "epoch :  24 ; Train_acc :  0.9386 ; Val_acc :  0.9029 ; Train_loss   0.1619 ; Val_loss   0.2838\n",
      "epoch :  25 ; Train_acc :  0.942 ; Val_acc :  0.9065 ; Train_loss   0.1529 ; Val_loss   0.289\n",
      "epoch :  26 ; Train_acc :  0.943 ; Val_acc :  0.9047 ; Train_loss   0.1504 ; Val_loss   0.2804\n",
      "epoch :  27 ; Train_acc :  0.9464 ; Val_acc :  0.9052 ; Train_loss   0.1429 ; Val_loss   0.289\n",
      "epoch :  28 ; Train_acc :  0.9492 ; Val_acc :  0.9068 ; Train_loss   0.135 ; Val_loss   0.2961\n",
      "epoch :  29 ; Train_acc :  0.9488 ; Val_acc :  0.9061 ; Train_loss   0.1377 ; Val_loss   0.2947\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = LSTM(input_size=28,\n",
    "             hidden_size=64,\n",
    "             num_layers=3,\n",
    "             output_size=10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, validate=True, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.7417 ; Train_loss   0.7122\n",
      "epoch =  1 ; Train_acc :  0.8301 ; Train_loss   0.4643\n",
      "epoch =  2 ; Train_acc :  0.8546 ; Train_loss   0.3926\n",
      "epoch =  3 ; Train_acc :  0.8692 ; Train_loss   0.3569\n",
      "epoch =  4 ; Train_acc :  0.8761 ; Train_loss   0.3346\n",
      "epoch =  5 ; Train_acc :  0.8834 ; Train_loss   0.3145\n",
      "epoch =  6 ; Train_acc :  0.8894 ; Train_loss   0.2981\n",
      "epoch =  7 ; Train_acc :  0.8941 ; Train_loss   0.2856\n",
      "epoch =  8 ; Train_acc :  0.9011 ; Train_loss   0.27\n",
      "epoch =  9 ; Train_acc :  0.9024 ; Train_loss   0.2592\n",
      "epoch =  10 ; Train_acc :  0.907 ; Train_loss   0.2486\n",
      "epoch =  11 ; Train_acc :  0.91 ; Train_loss   0.2394\n",
      "epoch =  12 ; Train_acc :  0.9139 ; Train_loss   0.231\n",
      "epoch =  13 ; Train_acc :  0.9171 ; Train_loss   0.2232\n",
      "epoch =  14 ; Train_acc :  0.9199 ; Train_loss   0.2153\n",
      "epoch =  15 ; Train_acc :  0.9233 ; Train_loss   0.2066\n",
      "epoch =  16 ; Train_acc :  0.9254 ; Train_loss   0.1984\n",
      "epoch =  17 ; Train_acc :  0.9283 ; Train_loss   0.1919\n",
      "epoch =  18 ; Train_acc :  0.9314 ; Train_loss   0.1833\n",
      "epoch =  19 ; Train_acc :  0.9337 ; Train_loss   0.1768\n",
      "epoch =  20 ; Train_acc :  0.9367 ; Train_loss   0.1713\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 21 epochs on full training set\n",
    "#Create Model\n",
    "Model = LSTM(input_size=28,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=10).to(device)\n",
    "\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, validate=False, max_epoch=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.2894720965728592\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.822688   0.863  0.842362\n",
      "1      1   0.977137   0.983  0.980060\n",
      "2      2   0.841286   0.811  0.825866\n",
      "3      3   0.900802   0.899  0.899900\n",
      "4      4   0.818006   0.845  0.831284\n",
      "5      5   0.980962   0.979  0.979980\n",
      "6      6   0.743697   0.708  0.725410\n",
      "7      7   0.946654   0.976  0.961103\n",
      "8      8   0.986882   0.978  0.982421\n",
      "9      9   0.975460   0.954  0.964611\n",
      "\n",
      "Accuracy  :  0.8996000289916992\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/LSTM_FMNIST.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Retrieve\n",
    "Modelx = LSTM(input_size=28,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=10).to(device)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/LSTM_FMNIST.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
