{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN for char level seq prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    \n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        #read from csv file:\n",
    "        data = []\n",
    "        with open(root_dir+txt_file, 'r') as file:\n",
    "            data.append(str(file.read().replace('\\n', '').replace('\\'', '').replace('\"','')))\n",
    "        #Preprocess data\n",
    "        data = list(data[0])\n",
    "        #make a dictionary of words\n",
    "        full_set = set([])\n",
    "        full_set = full_set.union(set(data)) \n",
    "        full_set = dict(enumerate(full_set))\n",
    "        self.full_set = {c: i for i, c in full_set.items()}\n",
    "        data = [self.full_set[i] for i in data]\n",
    "        self.data = np.array(data)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    #function to return length of data\n",
    "    def  __len__(self):\n",
    "        #we can take first 10 entries a s we will not be having last 10 features for them\n",
    "        return len(self.data)-10\n",
    "    \n",
    "    #function to get data\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx_label = idx+10\n",
    "        #for letter i, features are last 10 letter\n",
    "        idx_features = [idx_label-i for i in range (1,11)]\n",
    "       \n",
    "        #we assign next index as label for current sequence\n",
    "        sample = {'features' : [self.data[idx_features[i]] for i in range(10)],\n",
    "                     'label' : self.data[idx_label] \n",
    "                 }\n",
    "        \n",
    "        #apply transformation\n",
    "        if(self.transform):\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ToTensor class\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        feature = sample['features']\n",
    "        #one hot encoding of features\n",
    "        arr = np.zeros((len(feature),72))\n",
    "        for i in range(len(feature)):\n",
    "            arr[i,feature[i]] = 1\n",
    "            \n",
    "        label = sample['label']\n",
    "        \n",
    "        return [ torch.tensor(arr), torch.tensor(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain data\n",
    "data = MyData(txt_file='text_data.txt', \n",
    "              root_dir='./../0. Data/',\n",
    "              transform = T.Compose([\n",
    "                ToTensor()\n",
    "                ]))\n",
    "\n",
    "#we do not split data to train and test as it is a generative model with no fix target for any sequence\n",
    "data_loader = DataLoader(data,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  7,  2,  2, 46, 13, 29, 27,  2,  8, 17,  6, 26, 55, 27, 18, 18, 21,\n",
       "         2, 29, 18, 55, 17,  7, 44,  8, 18, 64, 27, 46, 55, 55])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vanilla RNN class\n",
    "class Vanilla_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(Vanilla_RNN, self).__init__()\n",
    "        #save variables to use in other functions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        #define RNN layer\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        #convert output to desired output dimension(readout layer)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #call RNN layer\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        #We will use only last output\n",
    "        out = self.fc(out[:,-1,:].view(x.shape[0],self.hidden_size))\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = data_loader\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "            \n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].float().to(device).view(len(sample[0]),10,72)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        #print losses in every epoch\n",
    "        print('epoch = ', epoch,'; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model, seq, l):\n",
    "    \n",
    "    # save sequence to output\n",
    "    out = [i for i in seq]\n",
    "    \n",
    "    #define dictionaries to convert\n",
    "    char2int = data.full_set\n",
    "    int2char = {i : c for c , i in char2int.items()}\n",
    "    \n",
    "    seq = [char2int[i] for i in seq]\n",
    "    features = torch.zeros(10,72)\n",
    "    #one hot encoding\n",
    "    for i in range(10):\n",
    "        features[i,seq[i]] = 1\n",
    "    \n",
    "    #we run this for l iteration, in each iteration, we get letter at position l+i\n",
    "    with torch.no_grad():\n",
    "        for i in range(l):\n",
    "            #set model to evaluation mode\n",
    "            Model.eval()\n",
    "            output = Model(features.to(device).view(1,10,72))\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #append  word to output\n",
    "            out.append(int2char[output.item()])\n",
    "            features = features[1:]\n",
    "            temp = torch.zeros(1,72)\n",
    "            temp[0,output] = 1\n",
    "            features = torch.cat((features,temp), dim=0)\n",
    "            \n",
    "        print(''.join([i for i in out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_loss   3.0453\n",
      "epoch =  1 ; Train_loss   2.8284\n",
      "epoch =  2 ; Train_loss   2.697\n",
      "epoch =  3 ; Train_loss   2.65\n",
      "epoch =  4 ; Train_loss   2.6303\n",
      "epoch =  5 ; Train_loss   2.6797\n",
      "epoch =  6 ; Train_loss   2.6469\n",
      "epoch =  7 ; Train_loss   2.5785\n",
      "epoch =  8 ; Train_loss   2.5419\n",
      "epoch =  9 ; Train_loss   2.5352\n",
      "epoch =  10 ; Train_loss   2.5321\n",
      "epoch =  11 ; Train_loss   2.5288\n",
      "epoch =  12 ; Train_loss   2.4495\n",
      "epoch =  13 ; Train_loss   2.3938\n",
      "epoch =  14 ; Train_loss   2.4209\n",
      "epoch =  15 ; Train_loss   2.3972\n",
      "epoch =  16 ; Train_loss   2.336\n",
      "epoch =  17 ; Train_loss   2.3233\n",
      "epoch =  18 ; Train_loss   2.2894\n",
      "epoch =  19 ; Train_loss   2.2605\n",
      "epoch =  20 ; Train_loss   2.235\n",
      "epoch =  21 ; Train_loss   2.2188\n",
      "epoch =  22 ; Train_loss   2.2163\n",
      "epoch =  23 ; Train_loss   2.197\n",
      "epoch =  24 ; Train_loss   2.2026\n",
      "epoch =  25 ; Train_loss   2.1845\n",
      "epoch =  26 ; Train_loss   2.163\n",
      "epoch =  27 ; Train_loss   2.1479\n",
      "epoch =  28 ; Train_loss   2.164\n",
      "epoch =  29 ; Train_loss   2.1596\n",
      "epoch =  30 ; Train_loss   2.1449\n",
      "epoch =  31 ; Train_loss   2.1757\n",
      "epoch =  32 ; Train_loss   2.1459\n",
      "epoch =  33 ; Train_loss   2.1179\n",
      "epoch =  34 ; Train_loss   2.1195\n",
      "epoch =  35 ; Train_loss   2.1103\n",
      "epoch =  36 ; Train_loss   2.0958\n",
      "epoch =  37 ; Train_loss   2.086\n",
      "epoch =  38 ; Train_loss   2.1062\n",
      "epoch =  39 ; Train_loss   2.0923\n",
      "epoch =  40 ; Train_loss   2.0797\n",
      "epoch =  41 ; Train_loss   2.07\n",
      "epoch =  42 ; Train_loss   2.0615\n",
      "epoch =  43 ; Train_loss   2.0457\n",
      "epoch =  44 ; Train_loss   2.0482\n",
      "epoch =  45 ; Train_loss   2.0447\n",
      "epoch =  46 ; Train_loss   2.0899\n",
      "epoch =  47 ; Train_loss   2.0448\n",
      "epoch =  48 ; Train_loss   2.0543\n",
      "epoch =  49 ; Train_loss   2.0377\n",
      "epoch =  50 ; Train_loss   2.0289\n",
      "epoch =  51 ; Train_loss   2.0359\n",
      "epoch =  52 ; Train_loss   2.0292\n",
      "epoch =  53 ; Train_loss   2.0388\n",
      "epoch =  54 ; Train_loss   2.0382\n",
      "epoch =  55 ; Train_loss   2.0497\n",
      "epoch =  56 ; Train_loss   2.0442\n",
      "epoch =  57 ; Train_loss   2.02\n",
      "epoch =  58 ; Train_loss   2.0046\n",
      "epoch =  59 ; Train_loss   2.0527\n",
      "epoch =  60 ; Train_loss   2.0172\n",
      "epoch =  61 ; Train_loss   2.0323\n",
      "epoch =  62 ; Train_loss   2.0366\n",
      "epoch =  63 ; Train_loss   2.0109\n",
      "epoch =  64 ; Train_loss   2.009\n",
      "epoch =  65 ; Train_loss   2.0111\n",
      "epoch =  66 ; Train_loss   1.9993\n",
      "epoch =  67 ; Train_loss   2.0194\n",
      "epoch =  68 ; Train_loss   2.0049\n",
      "epoch =  69 ; Train_loss   2.0093\n",
      "epoch =  70 ; Train_loss   2.0262\n",
      "epoch =  71 ; Train_loss   2.0369\n",
      "epoch =  72 ; Train_loss   2.0296\n",
      "epoch =  73 ; Train_loss   2.0156\n",
      "epoch =  74 ; Train_loss   2.0051\n",
      "epoch =  75 ; Train_loss   2.0134\n",
      "epoch =  76 ; Train_loss   2.0024\n",
      "epoch =  77 ; Train_loss   1.9959\n",
      "epoch =  78 ; Train_loss   2.0116\n",
      "epoch =  79 ; Train_loss   1.9989\n",
      "epoch =  80 ; Train_loss   2.0106\n",
      "epoch =  81 ; Train_loss   1.9996\n",
      "epoch =  82 ; Train_loss   2.0049\n",
      "epoch =  83 ; Train_loss   1.996\n",
      "epoch =  84 ; Train_loss   1.9843\n",
      "epoch =  85 ; Train_loss   2.0025\n",
      "epoch =  86 ; Train_loss   1.9839\n",
      "epoch =  87 ; Train_loss   2.0051\n",
      "epoch =  88 ; Train_loss   2.0005\n",
      "epoch =  89 ; Train_loss   2.0059\n",
      "epoch =  90 ; Train_loss   2.0016\n",
      "epoch =  91 ; Train_loss   1.9894\n",
      "epoch =  92 ; Train_loss   2.0049\n",
      "epoch =  93 ; Train_loss   1.9868\n",
      "epoch =  94 ; Train_loss   1.9775\n",
      "epoch =  95 ; Train_loss   1.976\n",
      "epoch =  96 ; Train_loss   1.9896\n",
      "epoch =  99 ; Train_loss   1.9743\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = Vanilla_RNN(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, max_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = ['You have s']\n",
    "seq2 = ['i will be ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have s rnt revcawedht  ehnar ehscre iec eee vws atr vteeasnaete   n  nnhvAWcost  eeneer hstrd n ete heotot\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model,list(seq1[0]),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will be ntenyislvt  d  n dehAcaWtot  e hneh hAcvte  atenkehvstned   ee  d oAAs ascotn nest ew ottr hw at eae\n"
     ]
    }
   ],
   "source": [
    "test(Model,list(seq2[0]),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprized by results, we have implemented a very basic model and language modelling is a very dificult task. It requires huge architectures to generate something meaningful. So, we move to NLP where we will learn how to represent natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/vanilla_RNN_char.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Retrieve\n",
    "Modelx = Vanilla_RNN(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/vanilla_RNN_char.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
