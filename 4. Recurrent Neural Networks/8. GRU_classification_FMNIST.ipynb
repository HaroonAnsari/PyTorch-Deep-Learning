{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU for classification on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's download data using torchvision\n",
    "trainset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = True, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "testset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = False, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "#split training data to training and validation  data\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "#Convert data to dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size = 32,\n",
    "                                        shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size = 32, \n",
    "                                         shuffle = True)\n",
    "\n",
    "full_train_set  = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create GRU class\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        #save variables to use in other functions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        #define GRU layer\n",
    "        self.gru = nn.GRU(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        #convert output to desired output dimension(readout layer)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #call GRU layer\n",
    "        x = x.view(x.shape[0],28,28)\n",
    "        out, _ = self.gru(x)\n",
    "        \n",
    "        #We will use only last output\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, validate, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = full_train_set\n",
    "        \n",
    "        if(validate):\n",
    "            loader = train_loader\n",
    "        \n",
    "        cnf_tr = torch.zeros(10,10)\n",
    "        cnf_val = torch.zeros(10,10)\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "\n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradiuents to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].to(device)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #calculate output by argmax\n",
    "                output = torch.argmax(output, 1)\n",
    "                #update entries in confusion matrix\n",
    "                for i in range(output.shape[0]):\n",
    "                    cnf_tr[output[i],sample[1][i]] +=1\n",
    "            \n",
    "        if(validate):\n",
    "            #Evaluate on validation data\n",
    "            with torch.no_grad():\n",
    "                #set model to evaluation mode\n",
    "                Model.eval()\n",
    "                #evaluate on tvaidation data\n",
    "                for i, sample in enumerate(val_loader):\n",
    "                    output = Model(sample[0].to(device))\n",
    "                    loss = loss_function(output, sample[1].to(device))\n",
    "                    Val_Loss.append(loss.item())\n",
    "                    #calculate output by argmax\n",
    "                    output = torch.argmax(output, 1)\n",
    "                    #update entries in confusion matrix\n",
    "                    for i in range(output.shape[0]):\n",
    "                        cnf_val[output[i],sample[1][i]] +=1\n",
    "                   \n",
    "        actual_count = torch.sum(cnf_tr, dim=0)\n",
    "        correct_pred = torch.tensor([cnf_tr[i,i] for i in range(10)])\n",
    "        A_tr = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        if(validate):\n",
    "            actual_count = torch.sum(cnf_val, dim=0)\n",
    "            correct_pred = torch.tensor([cnf_val[i,i] for i in range(10)])\n",
    "            A_val = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        #print losses in every epoch\n",
    "        if(validate):\n",
    "            print('epoch : ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Val_acc : ', np.round(A_val,4),  \n",
    "                  '; Train_loss  ',np.round(np.mean(Train_Loss),4),  '; Val_loss  ',np.round(np.mean(Val_Loss),4))\n",
    "        else:\n",
    "            print('epoch = ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function top evaluate model using performace metrices\n",
    "def evaluate(cnf):\n",
    "    actual_count = torch.sum(cnf, dim=0)\n",
    "    predicted_count = torch.sum(cnf, dim=1)\n",
    "    correct_pred = torch.tensor([cnf[i,i] for i in range(10)])\n",
    "    #Precision\n",
    "    precision = correct_pred/predicted_count\n",
    "    #Recall\n",
    "    recall = correct_pred/actual_count\n",
    "    #F1-Score\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    #Accuracy\n",
    "    Accuracy = torch.sum(correct_pred)/torch.sum(actual_count)\n",
    "    print('\\n',pd.DataFrame({'Class':[i for i in range(10)],\n",
    "                 'Precision' : precision,\n",
    "                 'Recall' : recall,\n",
    "                 'F1_Score': f1_score}))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy  : ', Accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model):\n",
    "    Loss = []\n",
    "    #confusion matrix\n",
    "    cnf = torch.zeros(10,10)\n",
    "\n",
    "    #evaluate on test data\n",
    "    with torch.no_grad():\n",
    "        #set model to evaluation mode\n",
    "        Model.eval()\n",
    "        #evaluate on test data\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            output = Model(sample[0].to(device))\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            Loss.append(loss.item())\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #update entries in confusion matrix\n",
    "            for i in range(output.shape[0]):\n",
    "                cnf[output[i],sample[1][i]] +=1\n",
    "\n",
    "        #print test loss\n",
    "        print('Test loss : ', np.mean(Loss))\n",
    "\n",
    "    #print evaluation summary\n",
    "    evaluate(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.7532 ; Val_acc :  0.8052 ; Train_loss   0.6806 ; Val_loss   0.5225\n",
      "epoch :  1 ; Train_acc :  0.8402 ; Val_acc :  0.8471 ; Train_loss   0.4352 ; Val_loss   0.4122\n",
      "epoch :  2 ; Train_acc :  0.8616 ; Val_acc :  0.8666 ; Train_loss   0.3731 ; Val_loss   0.3638\n",
      "epoch :  3 ; Train_acc :  0.8741 ; Val_acc :  0.873 ; Train_loss   0.339 ; Val_loss   0.3549\n",
      "epoch :  4 ; Train_acc :  0.8827 ; Val_acc :  0.878 ; Train_loss   0.315 ; Val_loss   0.3352\n",
      "epoch :  5 ; Train_acc :  0.8898 ; Val_acc :  0.8876 ; Train_loss   0.2975 ; Val_loss   0.3158\n",
      "epoch :  6 ; Train_acc :  0.8965 ; Val_acc :  0.8894 ; Train_loss   0.2779 ; Val_loss   0.308\n",
      "epoch :  7 ; Train_acc :  0.9004 ; Val_acc :  0.8941 ; Train_loss   0.2635 ; Val_loss   0.3006\n",
      "epoch :  8 ; Train_acc :  0.9064 ; Val_acc :  0.8928 ; Train_loss   0.2511 ; Val_loss   0.2973\n",
      "epoch :  9 ; Train_acc :  0.9103 ; Val_acc :  0.8956 ; Train_loss   0.2395 ; Val_loss   0.2908\n",
      "epoch :  10 ; Train_acc :  0.913 ; Val_acc :  0.8928 ; Train_loss   0.2304 ; Val_loss   0.2993\n",
      "epoch :  11 ; Train_acc :  0.9171 ; Val_acc :  0.903 ; Train_loss   0.2217 ; Val_loss   0.2752\n",
      "epoch :  12 ; Train_acc :  0.9214 ; Val_acc :  0.9001 ; Train_loss   0.2102 ; Val_loss   0.2916\n",
      "epoch :  13 ; Train_acc :  0.9244 ; Val_acc :  0.897 ; Train_loss   0.2027 ; Val_loss   0.2917\n",
      "epoch :  14 ; Train_acc :  0.9264 ; Val_acc :  0.9033 ; Train_loss   0.1938 ; Val_loss   0.2795\n",
      "epoch :  15 ; Train_acc :  0.9309 ; Val_acc :  0.9025 ; Train_loss   0.1857 ; Val_loss   0.2913\n",
      "epoch :  16 ; Train_acc :  0.933 ; Val_acc :  0.9051 ; Train_loss   0.1778 ; Val_loss   0.2805\n",
      "epoch :  17 ; Train_acc :  0.936 ; Val_acc :  0.9 ; Train_loss   0.1707 ; Val_loss   0.2945\n",
      "epoch :  18 ; Train_acc :  0.9385 ; Val_acc :  0.9029 ; Train_loss   0.1628 ; Val_loss   0.2917\n",
      "epoch :  19 ; Train_acc :  0.9413 ; Val_acc :  0.9037 ; Train_loss   0.1567 ; Val_loss   0.2918\n",
      "epoch :  20 ; Train_acc :  0.9445 ; Val_acc :  0.9003 ; Train_loss   0.1492 ; Val_loss   0.3003\n",
      "epoch :  21 ; Train_acc :  0.9465 ; Val_acc :  0.8996 ; Train_loss   0.1419 ; Val_loss   0.3118\n",
      "epoch :  22 ; Train_acc :  0.9486 ; Val_acc :  0.9065 ; Train_loss   0.1368 ; Val_loss   0.291\n",
      "epoch :  23 ; Train_acc :  0.9509 ; Val_acc :  0.9025 ; Train_loss   0.1294 ; Val_loss   0.3116\n",
      "epoch :  24 ; Train_acc :  0.9528 ; Val_acc :  0.9031 ; Train_loss   0.1247 ; Val_loss   0.3183\n",
      "epoch :  25 ; Train_acc :  0.9569 ; Val_acc :  0.8983 ; Train_loss   0.1161 ; Val_loss   0.3203\n",
      "epoch :  26 ; Train_acc :  0.9582 ; Val_acc :  0.9013 ; Train_loss   0.1109 ; Val_loss   0.3314\n",
      "epoch :  27 ; Train_acc :  0.9593 ; Val_acc :  0.8976 ; Train_loss   0.1084 ; Val_loss   0.345\n",
      "epoch :  28 ; Train_acc :  0.9626 ; Val_acc :  0.8999 ; Train_loss   0.1023 ; Val_loss   0.3441\n",
      "epoch :  29 ; Train_acc :  0.9637 ; Val_acc :  0.9005 ; Train_loss   0.0972 ; Val_loss   0.3494\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = GRU(input_size=28,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            output_size=10).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, validate=True, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.7628 ; Train_loss   0.6461\n",
      "epoch =  1 ; Train_acc :  0.8486 ; Train_loss   0.4107\n",
      "epoch =  2 ; Train_acc :  0.8663 ; Train_loss   0.3596\n",
      "epoch =  3 ; Train_acc :  0.8797 ; Train_loss   0.326\n",
      "epoch =  4 ; Train_acc :  0.8883 ; Train_loss   0.3024\n",
      "epoch =  5 ; Train_acc :  0.8948 ; Train_loss   0.2853\n",
      "epoch =  6 ; Train_acc :  0.9006 ; Train_loss   0.2691\n",
      "epoch =  7 ; Train_acc :  0.9046 ; Train_loss   0.2553\n",
      "epoch =  8 ; Train_acc :  0.9107 ; Train_loss   0.2411\n",
      "epoch =  9 ; Train_acc :  0.9146 ; Train_loss   0.2316\n",
      "epoch =  10 ; Train_acc :  0.9172 ; Train_loss   0.2221\n",
      "epoch =  11 ; Train_acc :  0.9214 ; Train_loss   0.2124\n",
      "epoch =  12 ; Train_acc :  0.9236 ; Train_loss   0.2033\n",
      "epoch =  13 ; Train_acc :  0.9283 ; Train_loss   0.1914\n",
      "epoch =  14 ; Train_acc :  0.9299 ; Train_loss   0.1872\n",
      "epoch =  15 ; Train_acc :  0.9344 ; Train_loss   0.1771\n",
      "epoch =  16 ; Train_acc :  0.9359 ; Train_loss   0.1713\n",
      "epoch =  17 ; Train_acc :  0.9398 ; Train_loss   0.1638\n",
      "epoch =  18 ; Train_acc :  0.9419 ; Train_loss   0.156\n",
      "epoch =  19 ; Train_acc :  0.9449 ; Train_loss   0.1482\n",
      "epoch =  20 ; Train_acc :  0.9466 ; Train_loss   0.143\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 21 epochs on full training set\n",
    "#Create Model\n",
    "Model = GRU(input_size=28,\n",
    "        hidden_size=64,\n",
    "        num_layers=3,\n",
    "        output_size=10).to(device)\n",
    "\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, validate=False, max_epoch=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.2945635337085008\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.855959   0.826  0.840712\n",
      "1      1   0.995889   0.969  0.982260\n",
      "2      2   0.845128   0.824  0.834430\n",
      "3      3   0.869202   0.937  0.901829\n",
      "4      4   0.831673   0.835  0.833333\n",
      "5      5   0.977023   0.978  0.977511\n",
      "6      6   0.738189   0.750  0.744048\n",
      "7      7   0.964575   0.953  0.958753\n",
      "8      8   0.986829   0.974  0.980372\n",
      "9      9   0.958539   0.971  0.964729\n",
      "\n",
      "Accuracy  :  0.9017000198364258\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/GRU_FMNIST.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Retrieve\n",
    "Modelx = GRU(input_size=28,\n",
    "        hidden_size=64,\n",
    "        num_layers=3,\n",
    "        output_size=10).to(device)\n",
    "\n",
    "Modelx.load_state_dict(torch.load('./saved_models/GRU_FMNIST.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
