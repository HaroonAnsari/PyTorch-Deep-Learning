{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU for char level seq prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    \n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        #read from csv file:\n",
    "        data = []\n",
    "        with open(root_dir+txt_file, 'r') as file:\n",
    "            data.append(str(file.read().replace('\\n', '').replace('\\'', '').replace('\"','')))\n",
    "        #Preprocess data\n",
    "        data = list(data[0])\n",
    "        #make a dictionary of words\n",
    "        full_set = set([])\n",
    "        full_set = full_set.union(set(data)) \n",
    "        full_set = dict(enumerate(full_set))\n",
    "        self.full_set = {c: i for i, c in full_set.items()}\n",
    "        data = [self.full_set[i] for i in data]\n",
    "        self.data = np.array(data)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    #function to return length of data\n",
    "    def  __len__(self):\n",
    "        #we can take first 10 entries a s we will not be having last 10 features for them\n",
    "        return len(self.data)-10\n",
    "    \n",
    "    #function to get data\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx_label = idx+10\n",
    "        #for letter i, features are last 10 letter\n",
    "        idx_features = [idx_label-i for i in range (1,11)]\n",
    "       \n",
    "        #we assign next index as label for current sequence\n",
    "        sample = {'features' : [self.data[idx_features[i]] for i in range(10)],\n",
    "                     'label' : self.data[idx_label] \n",
    "                 }\n",
    "        \n",
    "        #apply transformation\n",
    "        if(self.transform):\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ToTensor class\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        feature = sample['features']\n",
    "        #one hot encoding of features\n",
    "        arr = np.zeros((len(feature),72))\n",
    "        for i in range(len(feature)):\n",
    "            arr[i,feature[i]] = 1\n",
    "            \n",
    "        label = sample['label']\n",
    "        \n",
    "        return [ torch.tensor(arr), torch.tensor(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain data\n",
    "data = MyData(txt_file='text_data.txt', \n",
    "              root_dir='./../0. Data/',\n",
    "              transform = T.Compose([\n",
    "                ToTensor()\n",
    "                ]))\n",
    "\n",
    "#we do not split data to train and test as it is a generative model with no fix target for any sequence\n",
    "data_loader = DataLoader(data,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  5, 14, 38, 60,  4, 59, 32, 32, 21, 14, 11, 48, 48, 14, 38, 29, 32,\n",
       "        32, 49,  0, 23, 48, 59, 32, 29, 42, 23, 11, 33,  5, 39])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vanilla RNN class\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        #save variables to use in other functions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        #define GRU layer\n",
    "        self.gru = nn.GRU(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        #convert output to desired output dimension(readout layer)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #call GRU layer\n",
    "        out, _ = self.gru(x)\n",
    "        \n",
    "        #We will use only last output\n",
    "        out = self.fc(out[:,-1,:].view(x.shape[0],self.hidden_size))\n",
    "        return out\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = data_loader\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "            \n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(sample[0].float().to(device).view(len(sample[0]),10,72)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        #print losses in every epoch\n",
    "        print('epoch = ', epoch,'; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model, seq, l):\n",
    "    \n",
    "    # save sequence to output\n",
    "    out = [i for i in seq]\n",
    "    \n",
    "    #define dictionaries to convert\n",
    "    char2int = data.full_set\n",
    "    int2char = {i : c for c , i in char2int.items()}\n",
    "    \n",
    "    seq = [char2int[i] for i in seq]\n",
    "    features = torch.zeros(10,72)\n",
    "    #one hot encoding\n",
    "    for i in range(10):\n",
    "        features[i,seq[i]] = 1\n",
    "    \n",
    "    #we run this for l iteration, in each iteration, we get letter at position l+i\n",
    "    with torch.no_grad():\n",
    "        for i in range(l):\n",
    "            #set model to evaluation mode\n",
    "            Model.eval()\n",
    "            output = Model(features.to(device).view(1,10,72))\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #append  word to output\n",
    "            out.append(int2char[output.item()])\n",
    "            features = features[1:]\n",
    "            temp = torch.zeros(1,72)\n",
    "            temp[0,output] = 1\n",
    "            features = torch.cat((features,temp), dim=0)\n",
    "            \n",
    "        print(''.join([i for i in out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_loss   2.6869\n",
      "epoch =  1 ; Train_loss   2.3403\n",
      "epoch =  2 ; Train_loss   2.2527\n",
      "epoch =  3 ; Train_loss   2.1487\n",
      "epoch =  4 ; Train_loss   2.0732\n",
      "epoch =  5 ; Train_loss   2.0157\n",
      "epoch =  6 ; Train_loss   1.9681\n",
      "epoch =  7 ; Train_loss   1.9211\n",
      "epoch =  8 ; Train_loss   1.8835\n",
      "epoch =  9 ; Train_loss   1.8533\n",
      "epoch =  10 ; Train_loss   1.8188\n",
      "epoch =  11 ; Train_loss   1.789\n",
      "epoch =  12 ; Train_loss   1.7611\n",
      "epoch =  13 ; Train_loss   1.7349\n",
      "epoch =  14 ; Train_loss   1.712\n",
      "epoch =  15 ; Train_loss   1.6934\n",
      "epoch =  16 ; Train_loss   1.6814\n",
      "epoch =  17 ; Train_loss   1.6586\n",
      "epoch =  18 ; Train_loss   1.6404\n",
      "epoch =  19 ; Train_loss   1.6231\n",
      "epoch =  20 ; Train_loss   1.6113\n",
      "epoch =  21 ; Train_loss   1.5962\n",
      "epoch =  22 ; Train_loss   1.5851\n",
      "epoch =  23 ; Train_loss   1.573\n",
      "epoch =  24 ; Train_loss   1.5584\n",
      "epoch =  25 ; Train_loss   1.5547\n",
      "epoch =  26 ; Train_loss   1.5377\n",
      "epoch =  27 ; Train_loss   1.541\n",
      "epoch =  28 ; Train_loss   1.5224\n",
      "epoch =  29 ; Train_loss   1.5086\n",
      "epoch =  30 ; Train_loss   1.4988\n",
      "epoch =  31 ; Train_loss   1.491\n",
      "epoch =  32 ; Train_loss   1.4812\n",
      "epoch =  33 ; Train_loss   1.4721\n",
      "epoch =  34 ; Train_loss   1.4645\n",
      "epoch =  35 ; Train_loss   1.457\n",
      "epoch =  36 ; Train_loss   1.4474\n",
      "epoch =  37 ; Train_loss   1.4427\n",
      "epoch =  38 ; Train_loss   1.4351\n",
      "epoch =  39 ; Train_loss   1.4272\n",
      "epoch =  40 ; Train_loss   1.425\n",
      "epoch =  41 ; Train_loss   1.4135\n",
      "epoch =  42 ; Train_loss   1.406\n",
      "epoch =  43 ; Train_loss   1.3988\n",
      "epoch =  44 ; Train_loss   1.3933\n",
      "epoch =  45 ; Train_loss   1.3884\n",
      "epoch =  46 ; Train_loss   1.3795\n",
      "epoch =  47 ; Train_loss   1.3764\n",
      "epoch =  48 ; Train_loss   1.3679\n",
      "epoch =  49 ; Train_loss   1.3663\n",
      "epoch =  50 ; Train_loss   1.3546\n",
      "epoch =  51 ; Train_loss   1.3519\n",
      "epoch =  52 ; Train_loss   1.3497\n",
      "epoch =  53 ; Train_loss   1.3416\n",
      "epoch =  54 ; Train_loss   1.335\n",
      "epoch =  55 ; Train_loss   1.3342\n",
      "epoch =  56 ; Train_loss   1.3294\n",
      "epoch =  57 ; Train_loss   1.3203\n",
      "epoch =  58 ; Train_loss   1.317\n",
      "epoch =  59 ; Train_loss   1.3104\n",
      "epoch =  60 ; Train_loss   1.3069\n",
      "epoch =  61 ; Train_loss   1.2999\n",
      "epoch =  62 ; Train_loss   1.2935\n",
      "epoch =  63 ; Train_loss   1.2896\n",
      "epoch =  64 ; Train_loss   1.2885\n",
      "epoch =  65 ; Train_loss   1.2837\n",
      "epoch =  66 ; Train_loss   1.2765\n",
      "epoch =  67 ; Train_loss   1.2736\n",
      "epoch =  68 ; Train_loss   1.2712\n",
      "epoch =  69 ; Train_loss   1.2642\n",
      "epoch =  70 ; Train_loss   1.2615\n",
      "epoch =  71 ; Train_loss   1.2552\n",
      "epoch =  72 ; Train_loss   1.2542\n",
      "epoch =  73 ; Train_loss   1.246\n",
      "epoch =  74 ; Train_loss   1.2461\n",
      "epoch =  75 ; Train_loss   1.2372\n",
      "epoch =  76 ; Train_loss   1.2374\n",
      "epoch =  77 ; Train_loss   1.2314\n",
      "epoch =  78 ; Train_loss   1.2252\n",
      "epoch =  79 ; Train_loss   1.2262\n",
      "epoch =  80 ; Train_loss   1.2202\n",
      "epoch =  81 ; Train_loss   1.2217\n",
      "epoch =  82 ; Train_loss   1.2126\n",
      "epoch =  83 ; Train_loss   1.2098\n",
      "epoch =  84 ; Train_loss   1.2144\n",
      "epoch =  85 ; Train_loss   1.207\n",
      "epoch =  86 ; Train_loss   1.2114\n",
      "epoch =  87 ; Train_loss   1.2019\n",
      "epoch =  88 ; Train_loss   1.2008\n",
      "epoch =  89 ; Train_loss   1.1937\n",
      "epoch =  90 ; Train_loss   1.1926\n",
      "epoch =  91 ; Train_loss   1.1846\n",
      "epoch =  92 ; Train_loss   1.1833\n",
      "epoch =  93 ; Train_loss   1.1805\n",
      "epoch =  94 ; Train_loss   1.1807\n",
      "epoch =  95 ; Train_loss   1.1729\n",
      "epoch =  96 ; Train_loss   1.1786\n",
      "epoch =  97 ; Train_loss   1.1668\n",
      "epoch =  98 ; Train_loss   1.1692\n",
      "epoch =  99 ; Train_loss   1.1657\n",
      "epoch =  100 ; Train_loss   1.1635\n",
      "epoch =  101 ; Train_loss   1.1536\n",
      "epoch =  102 ; Train_loss   1.1562\n",
      "epoch =  103 ; Train_loss   1.1568\n",
      "epoch =  104 ; Train_loss   1.1535\n",
      "epoch =  105 ; Train_loss   1.1489\n",
      "epoch =  106 ; Train_loss   1.1436\n",
      "epoch =  107 ; Train_loss   1.1497\n",
      "epoch =  108 ; Train_loss   1.1406\n",
      "epoch =  109 ; Train_loss   1.1404\n",
      "epoch =  110 ; Train_loss   1.1351\n",
      "epoch =  111 ; Train_loss   1.1299\n",
      "epoch =  112 ; Train_loss   1.1296\n",
      "epoch =  113 ; Train_loss   1.1258\n",
      "epoch =  114 ; Train_loss   1.1272\n",
      "epoch =  115 ; Train_loss   1.125\n",
      "epoch =  116 ; Train_loss   1.1238\n",
      "epoch =  117 ; Train_loss   1.1223\n",
      "epoch =  118 ; Train_loss   1.1152\n",
      "epoch =  119 ; Train_loss   1.1151\n",
      "epoch =  120 ; Train_loss   1.1102\n",
      "epoch =  121 ; Train_loss   1.109\n",
      "epoch =  122 ; Train_loss   1.1083\n",
      "epoch =  123 ; Train_loss   1.1143\n",
      "epoch =  124 ; Train_loss   1.106\n",
      "epoch =  125 ; Train_loss   1.1083\n",
      "epoch =  126 ; Train_loss   1.106\n",
      "epoch =  127 ; Train_loss   1.0988\n",
      "epoch =  128 ; Train_loss   1.1002\n",
      "epoch =  129 ; Train_loss   1.1026\n",
      "epoch =  130 ; Train_loss   1.0936\n",
      "epoch =  131 ; Train_loss   1.0898\n",
      "epoch =  132 ; Train_loss   1.0925\n",
      "epoch =  133 ; Train_loss   1.0889\n",
      "epoch =  134 ; Train_loss   1.0858\n",
      "epoch =  135 ; Train_loss   1.0865\n",
      "epoch =  136 ; Train_loss   1.0847\n",
      "epoch =  137 ; Train_loss   1.0856\n",
      "epoch =  138 ; Train_loss   1.0872\n",
      "epoch =  139 ; Train_loss   1.074\n",
      "epoch =  140 ; Train_loss   1.0792\n",
      "epoch =  141 ; Train_loss   1.0856\n",
      "epoch =  142 ; Train_loss   1.0786\n",
      "epoch =  143 ; Train_loss   1.0761\n",
      "epoch =  144 ; Train_loss   1.0769\n",
      "epoch =  145 ; Train_loss   1.0668\n",
      "epoch =  146 ; Train_loss   1.0622\n",
      "epoch =  147 ; Train_loss   1.0671\n",
      "epoch =  148 ; Train_loss   1.074\n",
      "epoch =  149 ; Train_loss   1.0665\n",
      "epoch =  150 ; Train_loss   1.0641\n",
      "epoch =  151 ; Train_loss   1.0585\n",
      "epoch =  152 ; Train_loss   1.0565\n",
      "epoch =  153 ; Train_loss   1.0639\n",
      "epoch =  154 ; Train_loss   1.0625\n",
      "epoch =  155 ; Train_loss   1.0783\n",
      "epoch =  156 ; Train_loss   1.0614\n",
      "epoch =  157 ; Train_loss   1.0625\n",
      "epoch =  158 ; Train_loss   1.0534\n",
      "epoch =  159 ; Train_loss   1.0481\n",
      "epoch =  160 ; Train_loss   1.0485\n",
      "epoch =  161 ; Train_loss   1.0585\n",
      "epoch =  162 ; Train_loss   1.0503\n",
      "epoch =  163 ; Train_loss   1.0434\n",
      "epoch =  164 ; Train_loss   1.0557\n",
      "epoch =  165 ; Train_loss   1.0484\n",
      "epoch =  166 ; Train_loss   1.0481\n",
      "epoch =  167 ; Train_loss   1.0459\n",
      "epoch =  168 ; Train_loss   1.0489\n",
      "epoch =  169 ; Train_loss   1.0354\n",
      "epoch =  170 ; Train_loss   1.0408\n",
      "epoch =  171 ; Train_loss   1.032\n",
      "epoch =  172 ; Train_loss   1.0419\n",
      "epoch =  173 ; Train_loss   1.0375\n",
      "epoch =  174 ; Train_loss   1.0341\n",
      "epoch =  175 ; Train_loss   1.0376\n",
      "epoch =  176 ; Train_loss   1.0312\n",
      "epoch =  177 ; Train_loss   1.0348\n",
      "epoch =  178 ; Train_loss   1.039\n",
      "epoch =  179 ; Train_loss   1.0521\n",
      "epoch =  180 ; Train_loss   1.037\n",
      "epoch =  181 ; Train_loss   1.0283\n",
      "epoch =  182 ; Train_loss   1.0287\n",
      "epoch =  183 ; Train_loss   1.0325\n",
      "epoch =  184 ; Train_loss   1.0247\n",
      "epoch =  185 ; Train_loss   1.0288\n",
      "epoch =  186 ; Train_loss   1.0255\n",
      "epoch =  187 ; Train_loss   1.0267\n",
      "epoch =  188 ; Train_loss   1.0213\n",
      "epoch =  189 ; Train_loss   1.0261\n",
      "epoch =  190 ; Train_loss   1.0253\n",
      "epoch =  191 ; Train_loss   1.0226\n",
      "epoch =  192 ; Train_loss   1.0323\n",
      "epoch =  193 ; Train_loss   1.0197\n",
      "epoch =  194 ; Train_loss   1.0199\n",
      "epoch =  195 ; Train_loss   1.0427\n",
      "epoch =  196 ; Train_loss   1.0191\n",
      "epoch =  197 ; Train_loss   1.0139\n",
      "epoch =  198 ; Train_loss   1.0272\n",
      "epoch =  199 ; Train_loss   1.0297\n",
      "epoch =  200 ; Train_loss   1.0201\n",
      "epoch =  201 ; Train_loss   1.0203\n",
      "epoch =  202 ; Train_loss   1.0164\n",
      "epoch =  203 ; Train_loss   1.0171\n",
      "epoch =  204 ; Train_loss   1.01\n",
      "epoch =  205 ; Train_loss   1.012\n",
      "epoch =  206 ; Train_loss   1.0156\n",
      "epoch =  207 ; Train_loss   1.0048\n",
      "epoch =  208 ; Train_loss   1.0163\n",
      "epoch =  209 ; Train_loss   1.0091\n",
      "epoch =  210 ; Train_loss   1.0043\n",
      "epoch =  211 ; Train_loss   1.0207\n",
      "epoch =  212 ; Train_loss   1.0109\n",
      "epoch =  213 ; Train_loss   1.0037\n",
      "epoch =  214 ; Train_loss   1.0108\n",
      "epoch =  215 ; Train_loss   1.0165\n",
      "epoch =  216 ; Train_loss   0.9981\n",
      "epoch =  217 ; Train_loss   1.0055\n",
      "epoch =  218 ; Train_loss   1.0065\n",
      "epoch =  219 ; Train_loss   1.0033\n",
      "epoch =  220 ; Train_loss   1.0038\n",
      "epoch =  221 ; Train_loss   0.9975\n",
      "epoch =  222 ; Train_loss   0.9982\n",
      "epoch =  223 ; Train_loss   1.0014\n",
      "epoch =  224 ; Train_loss   1.0013\n",
      "epoch =  225 ; Train_loss   0.9921\n",
      "epoch =  226 ; Train_loss   0.9941\n",
      "epoch =  227 ; Train_loss   0.9997\n",
      "epoch =  228 ; Train_loss   0.9979\n",
      "epoch =  229 ; Train_loss   0.996\n",
      "epoch =  230 ; Train_loss   1.0097\n",
      "epoch =  231 ; Train_loss   0.9962\n",
      "epoch =  232 ; Train_loss   0.9882\n",
      "epoch =  233 ; Train_loss   1.0098\n",
      "epoch =  234 ; Train_loss   0.9923\n",
      "epoch =  235 ; Train_loss   0.9951\n",
      "epoch =  236 ; Train_loss   0.9997\n",
      "epoch =  237 ; Train_loss   0.9975\n",
      "epoch =  238 ; Train_loss   0.9879\n",
      "epoch =  239 ; Train_loss   0.9937\n",
      "epoch =  240 ; Train_loss   0.9909\n",
      "epoch =  241 ; Train_loss   1.0064\n",
      "epoch =  242 ; Train_loss   0.9792\n",
      "epoch =  243 ; Train_loss   0.9818\n",
      "epoch =  244 ; Train_loss   0.9848\n",
      "epoch =  245 ; Train_loss   0.9956\n",
      "epoch =  246 ; Train_loss   1.0053\n",
      "epoch =  247 ; Train_loss   0.997\n",
      "epoch =  248 ; Train_loss   0.994\n",
      "epoch =  249 ; Train_loss   0.9968\n",
      "epoch =  250 ; Train_loss   0.9881\n",
      "epoch =  251 ; Train_loss   0.9951\n",
      "epoch =  252 ; Train_loss   0.9865\n",
      "epoch =  253 ; Train_loss   0.997\n",
      "epoch =  254 ; Train_loss   0.9797\n",
      "epoch =  255 ; Train_loss   0.9871\n",
      "epoch =  256 ; Train_loss   0.9952\n",
      "epoch =  257 ; Train_loss   0.9903\n",
      "epoch =  258 ; Train_loss   0.9828\n",
      "epoch =  259 ; Train_loss   0.9816\n",
      "epoch =  260 ; Train_loss   0.9819\n",
      "epoch =  261 ; Train_loss   0.9917\n",
      "epoch =  262 ; Train_loss   0.9861\n",
      "epoch =  263 ; Train_loss   0.9807\n",
      "epoch =  264 ; Train_loss   0.9856\n",
      "epoch =  265 ; Train_loss   0.9884\n",
      "epoch =  266 ; Train_loss   0.9794\n",
      "epoch =  267 ; Train_loss   0.9834\n",
      "epoch =  268 ; Train_loss   0.9799\n",
      "epoch =  269 ; Train_loss   0.9709\n",
      "epoch =  270 ; Train_loss   0.9859\n",
      "epoch =  271 ; Train_loss   0.9883\n",
      "epoch =  272 ; Train_loss   0.9814\n",
      "epoch =  273 ; Train_loss   0.9891\n",
      "epoch =  274 ; Train_loss   0.9724\n",
      "epoch =  275 ; Train_loss   0.9787\n",
      "epoch =  276 ; Train_loss   0.9866\n",
      "epoch =  277 ; Train_loss   0.989\n",
      "epoch =  278 ; Train_loss   0.9718\n",
      "epoch =  279 ; Train_loss   0.983\n",
      "epoch =  280 ; Train_loss   0.9868\n",
      "epoch =  281 ; Train_loss   0.9899\n",
      "epoch =  282 ; Train_loss   0.9682\n",
      "epoch =  283 ; Train_loss   0.9656\n",
      "epoch =  284 ; Train_loss   0.9837\n",
      "epoch =  285 ; Train_loss   0.9658\n",
      "epoch =  286 ; Train_loss   0.9796\n",
      "epoch =  287 ; Train_loss   0.9685\n",
      "epoch =  288 ; Train_loss   0.9659\n",
      "epoch =  289 ; Train_loss   0.9718\n",
      "epoch =  290 ; Train_loss   0.9733\n",
      "epoch =  291 ; Train_loss   0.9811\n",
      "epoch =  292 ; Train_loss   0.9766\n",
      "epoch =  293 ; Train_loss   0.9785\n",
      "epoch =  294 ; Train_loss   0.9883\n",
      "epoch =  295 ; Train_loss   0.9794\n",
      "epoch =  296 ; Train_loss   0.9766\n",
      "epoch =  297 ; Train_loss   0.9754\n",
      "epoch =  298 ; Train_loss   0.9683\n",
      "epoch =  299 ; Train_loss   0.9711\n",
      "epoch =  300 ; Train_loss   0.9715\n",
      "epoch =  301 ; Train_loss   0.972\n",
      "epoch =  302 ; Train_loss   0.9723\n",
      "epoch =  303 ; Train_loss   0.9788\n",
      "epoch =  304 ; Train_loss   0.9706\n",
      "epoch =  305 ; Train_loss   0.9761\n",
      "epoch =  306 ; Train_loss   0.9764\n",
      "epoch =  307 ; Train_loss   0.9742\n",
      "epoch =  308 ; Train_loss   0.9607\n",
      "epoch =  309 ; Train_loss   0.9715\n",
      "epoch =  310 ; Train_loss   0.98\n",
      "epoch =  311 ; Train_loss   0.9797\n",
      "epoch =  312 ; Train_loss   0.9652\n",
      "epoch =  313 ; Train_loss   0.9717\n",
      "epoch =  314 ; Train_loss   0.9628\n",
      "epoch =  315 ; Train_loss   0.9643\n",
      "epoch =  316 ; Train_loss   0.9743\n",
      "epoch =  317 ; Train_loss   0.9682\n",
      "epoch =  318 ; Train_loss   0.9595\n",
      "epoch =  319 ; Train_loss   0.9732\n",
      "epoch =  320 ; Train_loss   0.9735\n",
      "epoch =  321 ; Train_loss   0.9624\n",
      "epoch =  322 ; Train_loss   0.9716\n",
      "epoch =  323 ; Train_loss   0.9591\n",
      "epoch =  324 ; Train_loss   0.9634\n",
      "epoch =  325 ; Train_loss   0.979\n",
      "epoch =  326 ; Train_loss   0.9584\n",
      "epoch =  327 ; Train_loss   0.9611\n",
      "epoch =  328 ; Train_loss   0.9671\n",
      "epoch =  329 ; Train_loss   0.96\n",
      "epoch =  330 ; Train_loss   0.9525\n",
      "epoch =  331 ; Train_loss   0.9722\n",
      "epoch =  332 ; Train_loss   0.9602\n",
      "epoch =  333 ; Train_loss   0.9792\n",
      "epoch =  334 ; Train_loss   0.9582\n",
      "epoch =  335 ; Train_loss   0.9764\n",
      "epoch =  336 ; Train_loss   0.9583\n",
      "epoch =  337 ; Train_loss   0.9625\n",
      "epoch =  338 ; Train_loss   0.9756\n",
      "epoch =  339 ; Train_loss   0.9781\n",
      "epoch =  340 ; Train_loss   0.9715\n",
      "epoch =  341 ; Train_loss   0.9622\n",
      "epoch =  342 ; Train_loss   0.964\n",
      "epoch =  343 ; Train_loss   0.9652\n",
      "epoch =  344 ; Train_loss   0.9625\n",
      "epoch =  345 ; Train_loss   0.9766\n",
      "epoch =  346 ; Train_loss   0.9809\n",
      "epoch =  347 ; Train_loss   0.9875\n",
      "epoch =  348 ; Train_loss   0.9783\n",
      "epoch =  349 ; Train_loss   0.9637\n",
      "epoch =  350 ; Train_loss   0.9726\n",
      "epoch =  351 ; Train_loss   0.9631\n",
      "epoch =  352 ; Train_loss   0.9683\n",
      "epoch =  353 ; Train_loss   0.9632\n",
      "epoch =  354 ; Train_loss   0.9748\n",
      "epoch =  355 ; Train_loss   0.972\n",
      "epoch =  356 ; Train_loss   0.9925\n",
      "epoch =  357 ; Train_loss   0.9961\n",
      "epoch =  358 ; Train_loss   0.9614\n",
      "epoch =  359 ; Train_loss   0.9676\n",
      "epoch =  360 ; Train_loss   0.9657\n",
      "epoch =  361 ; Train_loss   0.9632\n",
      "epoch =  362 ; Train_loss   0.9708\n",
      "epoch =  363 ; Train_loss   0.9746\n",
      "epoch =  364 ; Train_loss   0.9682\n",
      "epoch =  365 ; Train_loss   0.9704\n",
      "epoch =  366 ; Train_loss   0.9698\n",
      "epoch =  367 ; Train_loss   0.9616\n",
      "epoch =  368 ; Train_loss   1.0034\n",
      "epoch =  369 ; Train_loss   0.972\n",
      "epoch =  370 ; Train_loss   0.965\n",
      "epoch =  371 ; Train_loss   0.972\n",
      "epoch =  372 ; Train_loss   0.9677\n",
      "epoch =  373 ; Train_loss   0.9768\n",
      "epoch =  374 ; Train_loss   0.9676\n",
      "epoch =  375 ; Train_loss   0.9694\n",
      "epoch =  376 ; Train_loss   0.9871\n",
      "epoch =  377 ; Train_loss   0.9951\n",
      "epoch =  378 ; Train_loss   0.9647\n",
      "epoch =  379 ; Train_loss   0.9609\n",
      "epoch =  380 ; Train_loss   0.9728\n",
      "epoch =  381 ; Train_loss   0.9779\n",
      "epoch =  382 ; Train_loss   0.9622\n",
      "epoch =  383 ; Train_loss   0.9545\n",
      "epoch =  384 ; Train_loss   0.965\n",
      "epoch =  385 ; Train_loss   0.9595\n",
      "epoch =  386 ; Train_loss   0.9677\n",
      "epoch =  387 ; Train_loss   0.9652\n",
      "epoch =  388 ; Train_loss   0.9591\n",
      "epoch =  389 ; Train_loss   0.9618\n",
      "epoch =  390 ; Train_loss   0.948\n",
      "epoch =  391 ; Train_loss   0.9733\n",
      "epoch =  392 ; Train_loss   0.9655\n",
      "epoch =  393 ; Train_loss   0.9857\n",
      "epoch =  394 ; Train_loss   0.9636\n",
      "epoch =  395 ; Train_loss   0.9665\n",
      "epoch =  396 ; Train_loss   0.9606\n",
      "epoch =  397 ; Train_loss   0.9625\n",
      "epoch =  398 ; Train_loss   0.9598\n",
      "epoch =  399 ; Train_loss   0.9672\n",
      "epoch =  400 ; Train_loss   0.9499\n",
      "epoch =  401 ; Train_loss   0.9704\n",
      "epoch =  402 ; Train_loss   0.9659\n",
      "epoch =  403 ; Train_loss   0.9832\n",
      "epoch =  404 ; Train_loss   0.9623\n",
      "epoch =  405 ; Train_loss   0.9796\n",
      "epoch =  406 ; Train_loss   0.9864\n",
      "epoch =  407 ; Train_loss   0.9739\n",
      "epoch =  408 ; Train_loss   1.0068\n",
      "epoch =  409 ; Train_loss   0.9886\n",
      "epoch =  410 ; Train_loss   0.9638\n",
      "epoch =  411 ; Train_loss   0.9841\n",
      "epoch =  412 ; Train_loss   0.9673\n",
      "epoch =  413 ; Train_loss   0.9653\n",
      "epoch =  414 ; Train_loss   0.9634\n",
      "epoch =  415 ; Train_loss   0.9818\n",
      "epoch =  416 ; Train_loss   0.9809\n",
      "epoch =  417 ; Train_loss   0.9631\n",
      "epoch =  418 ; Train_loss   0.9886\n",
      "epoch =  419 ; Train_loss   0.9682\n",
      "epoch =  420 ; Train_loss   0.9699\n",
      "epoch =  421 ; Train_loss   0.9707\n",
      "epoch =  422 ; Train_loss   0.974\n",
      "epoch =  423 ; Train_loss   0.9722\n",
      "epoch =  424 ; Train_loss   0.9698\n",
      "epoch =  425 ; Train_loss   0.9789\n",
      "epoch =  426 ; Train_loss   0.9643\n",
      "epoch =  427 ; Train_loss   0.9698\n",
      "epoch =  428 ; Train_loss   0.9714\n",
      "epoch =  429 ; Train_loss   0.9733\n",
      "epoch =  430 ; Train_loss   0.9676\n",
      "epoch =  431 ; Train_loss   0.9735\n",
      "epoch =  432 ; Train_loss   0.9515\n",
      "epoch =  433 ; Train_loss   0.9634\n",
      "epoch =  434 ; Train_loss   0.9652\n",
      "epoch =  435 ; Train_loss   0.9621\n",
      "epoch =  436 ; Train_loss   0.9821\n",
      "epoch =  437 ; Train_loss   0.954\n",
      "epoch =  438 ; Train_loss   0.9504\n",
      "epoch =  439 ; Train_loss   0.9823\n",
      "epoch =  440 ; Train_loss   0.9599\n",
      "epoch =  441 ; Train_loss   0.9654\n",
      "epoch =  442 ; Train_loss   0.9663\n",
      "epoch =  443 ; Train_loss   0.9678\n",
      "epoch =  444 ; Train_loss   0.9605\n",
      "epoch =  445 ; Train_loss   0.964\n",
      "epoch =  446 ; Train_loss   0.9744\n",
      "epoch =  447 ; Train_loss   0.9592\n",
      "epoch =  448 ; Train_loss   0.9569\n",
      "epoch =  449 ; Train_loss   0.9546\n",
      "epoch =  450 ; Train_loss   0.968\n",
      "epoch =  451 ; Train_loss   0.9721\n",
      "epoch =  452 ; Train_loss   0.972\n",
      "epoch =  453 ; Train_loss   0.968\n",
      "epoch =  454 ; Train_loss   0.9557\n",
      "epoch =  455 ; Train_loss   0.9668\n",
      "epoch =  456 ; Train_loss   0.9549\n",
      "epoch =  457 ; Train_loss   0.9695\n",
      "epoch =  458 ; Train_loss   0.9428\n",
      "epoch =  459 ; Train_loss   0.9666\n",
      "epoch =  460 ; Train_loss   0.9508\n",
      "epoch =  461 ; Train_loss   0.9516\n",
      "epoch =  462 ; Train_loss   0.9646\n",
      "epoch =  463 ; Train_loss   0.9691\n",
      "epoch =  464 ; Train_loss   0.9599\n",
      "epoch =  465 ; Train_loss   0.9556\n",
      "epoch =  466 ; Train_loss   0.9485\n",
      "epoch =  467 ; Train_loss   0.9577\n",
      "epoch =  468 ; Train_loss   0.9651\n",
      "epoch =  469 ; Train_loss   0.9481\n",
      "epoch =  470 ; Train_loss   0.9471\n",
      "epoch =  471 ; Train_loss   0.9594\n",
      "epoch =  472 ; Train_loss   0.9555\n",
      "epoch =  473 ; Train_loss   0.9677\n",
      "epoch =  474 ; Train_loss   0.9742\n",
      "epoch =  475 ; Train_loss   0.9671\n",
      "epoch =  476 ; Train_loss   0.9543\n",
      "epoch =  477 ; Train_loss   0.9542\n",
      "epoch =  478 ; Train_loss   0.9639\n",
      "epoch =  479 ; Train_loss   0.9581\n",
      "epoch =  480 ; Train_loss   0.9652\n",
      "epoch =  481 ; Train_loss   0.9901\n",
      "epoch =  482 ; Train_loss   0.9572\n",
      "epoch =  483 ; Train_loss   0.9735\n",
      "epoch =  484 ; Train_loss   0.9683\n",
      "epoch =  485 ; Train_loss   0.9647\n",
      "epoch =  486 ; Train_loss   0.9647\n",
      "epoch =  487 ; Train_loss   0.969\n",
      "epoch =  488 ; Train_loss   0.9747\n",
      "epoch =  489 ; Train_loss   0.9612\n",
      "epoch =  490 ; Train_loss   0.9635\n",
      "epoch =  491 ; Train_loss   0.9584\n",
      "epoch =  492 ; Train_loss   0.9556\n",
      "epoch =  493 ; Train_loss   0.9635\n",
      "epoch =  494 ; Train_loss   0.9476\n",
      "epoch =  495 ; Train_loss   0.9571\n",
      "epoch =  496 ; Train_loss   0.9583\n",
      "epoch =  497 ; Train_loss   0.9466\n",
      "epoch =  498 ; Train_loss   0.9717\n",
      "epoch =  499 ; Train_loss   0.9613\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = GRU(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, max_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = ['You have s']\n",
    "seq2 = ['day and ni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have stusaileno hr gny efwoefaioylo.dn lnn eu oofieeypndkrandd etietneooidond, vknnewt, aeietlu  wrvnretpt\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model,list(seq1[0]),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day and nie.oNrtoCaga , oankiasa tkr  ds nteiutteeo  tnr  drupoeeuoa ,lprddgnna-eui, lodn-dpd trnuo-eaool grf-\n"
     ]
    }
   ],
   "source": [
    "test(Model,list(seq2[0]),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprized by results, we have implemented a very basic model and language modelling is a very dificult task. It requires huge architectures to generate something meaningful. So, we move to NLP where we will learn how to represent natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/GRU_char.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Retrieve\n",
    "Modelx = GRU(input_size=72,\n",
    "                    hidden_size=64,\n",
    "                    num_layers=3,\n",
    "                    output_size=72).to(device)\n",
    "Modelx.load_state_dict(torch.load('./saved_models/GRU_char.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
