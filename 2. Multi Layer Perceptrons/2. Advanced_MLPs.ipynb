{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced MLPs\n",
    "\n",
    "### Dropout\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "### Combined BN, Dropout and DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout is a very popular technique that act as ensemble method for neural networks. For tis, we just need to add one dropout layer before every fully connected layer. Dropout percentage is something that need to be tuned\n",
    "\n",
    "### Batch Normalization\n",
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. To implement this, we need to add a batch norm layer efor every fully connected layer.\n",
    "\n",
    "### Data Augmentation\n",
    "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. Let's now augment or train dataset by randomly cropping images. We write a function that performs a series of operations on data and call it before every iteration wit prob 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's download data using torchvision\n",
    "trainset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = True, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "testset = datasets.FashionMNIST('./../0. Data/', \n",
    "                                 download = True, \n",
    "                                 train = False, \n",
    "                                 transform = T.Compose([\n",
    "                                     T.ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "#split training data to training and validation  data\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "#Convert data to dataloader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size = 32,\n",
    "                                        shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size = 32, \n",
    "                                         shuffle = True)\n",
    "\n",
    "full_train_set  = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size = 32, \n",
    "                                          shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a MLP class defining our neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_len, output_len, d_out, b_norm):\n",
    "        super(MLP, self).__init__()\n",
    "        #three fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=input_len, out_features=512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "        self.b_norm = b_norm\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #convert image to a one dimentional tensor before feeding to neural network\n",
    "        x = x.flatten(start_dim=1)\n",
    "        #activation function is relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if(self.b_norm):\n",
    "            x = self.bn1(x)\n",
    "        #dropout\n",
    "        if(self.d_out):\n",
    "            x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if(self.b_norm):\n",
    "            x = self.bn2(x)\n",
    "        #dropout\n",
    "        if(self.d_out):\n",
    "            x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function for data augmentation\n",
    "dataAugment = T.Compose([T.ToPILImage(), T.Resize(32), T.RandomCrop(28), T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train(Model, aug, validate, max_epoch):\n",
    "    for epoch in range(max_epoch):\n",
    "        Train_Loss = []\n",
    "        Val_Loss =[]\n",
    "        loader = full_train_set\n",
    "        \n",
    "        if(validate):\n",
    "            loader = train_loader\n",
    "        \n",
    "        cnf_tr = torch.zeros(10,10)\n",
    "        cnf_val = torch.zeros(10,10)\n",
    "        \n",
    "        #Train on training data\n",
    "        for i, sample in enumerate(loader):\n",
    "            #data Augmentation\n",
    "            d = []\n",
    "            if(torch.rand(1)<0.5 and aug):\n",
    "                for t in sample[0]:\n",
    "                    d.append(dataAugment(t))\n",
    "                d = torch.stack((d))\n",
    "            else:\n",
    "                d = sample[0]\n",
    "\n",
    "            #set model to train mode\n",
    "            Model.train()\n",
    "            #set gradiuents to zero\n",
    "            optimizer.zero_grad()\n",
    "            #obtain output\n",
    "            output = Model(d.to(device)).to(device)\n",
    "            #compute loss\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "            #optimize weights\n",
    "            optimizer.step()\n",
    "            #record train loss\n",
    "            Train_Loss.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                #calculate output by argmax\n",
    "                output = torch.argmax(output, 1)\n",
    "                #update entries in confusion matrix\n",
    "                for i in range(output.shape[0]):\n",
    "                    cnf_tr[output[i],sample[1][i]] +=1\n",
    "            \n",
    "        if(validate):\n",
    "            #Evaluate on validation data\n",
    "            with torch.no_grad():\n",
    "                #set model to evaluation mode\n",
    "                Model.eval()\n",
    "                #evaluate on tvaidation data\n",
    "                for i, sample in enumerate(val_loader):\n",
    "                    output = Model(sample[0].to(device))\n",
    "                    loss = loss_function(output, sample[1].to(device))\n",
    "                    Val_Loss.append(loss.item())\n",
    "                    #calculate output by argmax\n",
    "                    output = torch.argmax(output, 1)\n",
    "                    #update entries in confusion matrix\n",
    "                    for i in range(output.shape[0]):\n",
    "                        cnf_val[output[i],sample[1][i]] +=1\n",
    "                   \n",
    "        actual_count = torch.sum(cnf_tr, dim=0)\n",
    "        correct_pred = torch.tensor([cnf_tr[i,i] for i in range(10)])\n",
    "        A_tr = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        if(validate):\n",
    "            actual_count = torch.sum(cnf_val, dim=0)\n",
    "            correct_pred = torch.tensor([cnf_val[i,i] for i in range(10)])\n",
    "            A_val = (torch.sum(correct_pred)/torch.sum(actual_count)).item()\n",
    "        \n",
    "        #print losses in every epoch\n",
    "        if(validate):\n",
    "            print('epoch : ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Val_acc : ', np.round(A_val,4),  \n",
    "                  '; Train_loss  ',np.round(np.mean(Train_Loss),4),  '; Val_loss  ',np.round(np.mean(Val_Loss),4))\n",
    "        else:\n",
    "            print('epoch = ',epoch,'; Train_acc : ', np.round(A_tr,4), '; Train_loss  ',np.round(np.mean(Train_Loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function top evaluate model using performace metrices\n",
    "def evaluate(cnf):\n",
    "    actual_count = torch.sum(cnf, dim=0)\n",
    "    predicted_count = torch.sum(cnf, dim=1)\n",
    "    correct_pred = torch.tensor([cnf[i,i] for i in range(10)])\n",
    "    #Precision\n",
    "    precision = correct_pred/predicted_count\n",
    "    #Recall\n",
    "    recall = correct_pred/actual_count\n",
    "    #F1-Score\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    #Accuracy\n",
    "    Accuracy = torch.sum(correct_pred)/torch.sum(actual_count)\n",
    "    print('\\n',pd.DataFrame({'Class':[i for i in range(10)],\n",
    "                 'Precision' : precision,\n",
    "                 'Recall' : recall,\n",
    "                 'F1_Score': f1_score}))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy  : ', Accuracy.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test model\n",
    "def test(Model):\n",
    "    Loss = []\n",
    "    #confusion matrix\n",
    "    cnf = torch.zeros(10,10)\n",
    "\n",
    "    #evaluate on test data\n",
    "    with torch.no_grad():\n",
    "        #set model to evaluation mode\n",
    "        Model.eval()\n",
    "        #evaluate on test data\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            output = Model(sample[0].to(device))\n",
    "            loss = loss_function(output, sample[1].to(device))\n",
    "            Loss.append(loss.item())\n",
    "            #calculate output by argmax\n",
    "            output = torch.argmax(output, 1)\n",
    "            #update entries in confusion matrix\n",
    "            for i in range(output.shape[0]):\n",
    "                cnf[output[i],sample[1][i]] +=1\n",
    "\n",
    "        #print test loss\n",
    "        print('Test loss : ', np.mean(Loss))\n",
    "\n",
    "    #print evaluation summary\n",
    "    evaluate(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.7862 ; Val_acc :  0.8367 ; Train_loss   0.5842 ; Val_loss   0.4416\n",
      "epoch :  1 ; Train_acc :  0.8373 ; Val_acc :  0.8541 ; Train_loss   0.4477 ; Val_loss   0.4008\n",
      "epoch :  2 ; Train_acc :  0.8462 ; Val_acc :  0.8645 ; Train_loss   0.4182 ; Val_loss   0.3713\n",
      "epoch :  3 ; Train_acc :  0.855 ; Val_acc :  0.8752 ; Train_loss   0.3954 ; Val_loss   0.3477\n",
      "epoch :  4 ; Train_acc :  0.8613 ; Val_acc :  0.8792 ; Train_loss   0.3812 ; Val_loss   0.3369\n",
      "epoch :  5 ; Train_acc :  0.8645 ; Val_acc :  0.8739 ; Train_loss   0.3704 ; Val_loss   0.3419\n",
      "epoch :  6 ; Train_acc :  0.8663 ; Val_acc :  0.8731 ; Train_loss   0.3629 ; Val_loss   0.3428\n",
      "epoch :  7 ; Train_acc :  0.8684 ; Val_acc :  0.8683 ; Train_loss   0.3555 ; Val_loss   0.3483\n",
      "epoch :  8 ; Train_acc :  0.8732 ; Val_acc :  0.8822 ; Train_loss   0.3444 ; Val_loss   0.3271\n",
      "epoch :  9 ; Train_acc :  0.8759 ; Val_acc :  0.8872 ; Train_loss   0.3387 ; Val_loss   0.3154\n",
      "epoch :  10 ; Train_acc :  0.8761 ; Val_acc :  0.8788 ; Train_loss   0.3349 ; Val_loss   0.3349\n",
      "epoch :  11 ; Train_acc :  0.8776 ; Val_acc :  0.8827 ; Train_loss   0.3327 ; Val_loss   0.3177\n",
      "epoch :  12 ; Train_acc :  0.8807 ; Val_acc :  0.8815 ; Train_loss   0.3237 ; Val_loss   0.329\n",
      "epoch :  13 ; Train_acc :  0.8821 ; Val_acc :  0.887 ; Train_loss   0.3212 ; Val_loss   0.3129\n",
      "epoch :  14 ; Train_acc :  0.884 ; Val_acc :  0.8833 ; Train_loss   0.3187 ; Val_loss   0.3164\n",
      "epoch :  15 ; Train_acc :  0.8843 ; Val_acc :  0.883 ; Train_loss   0.3136 ; Val_loss   0.3203\n",
      "epoch :  16 ; Train_acc :  0.8855 ; Val_acc :  0.8861 ; Train_loss   0.3116 ; Val_loss   0.3275\n",
      "epoch :  17 ; Train_acc :  0.8876 ; Val_acc :  0.8832 ; Train_loss   0.3058 ; Val_loss   0.3237\n",
      "epoch :  18 ; Train_acc :  0.8893 ; Val_acc :  0.8816 ; Train_loss   0.3044 ; Val_loss   0.3192\n",
      "epoch :  19 ; Train_acc :  0.8886 ; Val_acc :  0.8906 ; Train_loss   0.2992 ; Val_loss   0.3058\n",
      "epoch :  20 ; Train_acc :  0.8899 ; Val_acc :  0.8855 ; Train_loss   0.2974 ; Val_loss   0.3183\n",
      "epoch :  21 ; Train_acc :  0.8907 ; Val_acc :  0.8889 ; Train_loss   0.2961 ; Val_loss   0.3136\n",
      "epoch :  22 ; Train_acc :  0.8922 ; Val_acc :  0.8898 ; Train_loss   0.2929 ; Val_loss   0.3101\n",
      "epoch :  23 ; Train_acc :  0.8928 ; Val_acc :  0.8845 ; Train_loss   0.2919 ; Val_loss   0.3246\n",
      "epoch :  24 ; Train_acc :  0.894 ; Val_acc :  0.886 ; Train_loss   0.2919 ; Val_loss   0.3166\n",
      "epoch :  25 ; Train_acc :  0.8962 ; Val_acc :  0.8929 ; Train_loss   0.2857 ; Val_loss   0.3136\n",
      "epoch :  26 ; Train_acc :  0.8955 ; Val_acc :  0.8876 ; Train_loss   0.2845 ; Val_loss   0.3183\n",
      "epoch :  27 ; Train_acc :  0.8969 ; Val_acc :  0.8838 ; Train_loss   0.2823 ; Val_loss   0.3179\n",
      "epoch :  28 ; Train_acc :  0.8973 ; Val_acc :  0.8882 ; Train_loss   0.2801 ; Val_loss   0.3161\n",
      "epoch :  29 ; Train_acc :  0.8974 ; Val_acc :  0.8946 ; Train_loss   0.2789 ; Val_loss   0.3089\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = MLP(784,10,d_out=True, b_norm=False).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, aug=False, validate=True, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.7929 ; Train_loss   0.5684\n",
      "epoch =  1 ; Train_acc :  0.84 ; Train_loss   0.4404\n",
      "epoch =  2 ; Train_acc :  0.8516 ; Train_loss   0.4077\n",
      "epoch =  3 ; Train_acc :  0.8554 ; Train_loss   0.3944\n",
      "epoch =  4 ; Train_acc :  0.863 ; Train_loss   0.3761\n",
      "epoch =  5 ; Train_acc :  0.8647 ; Train_loss   0.3694\n",
      "epoch =  6 ; Train_acc :  0.8694 ; Train_loss   0.3595\n",
      "epoch =  7 ; Train_acc :  0.8716 ; Train_loss   0.3516\n",
      "epoch =  8 ; Train_acc :  0.8725 ; Train_loss   0.3446\n",
      "epoch =  9 ; Train_acc :  0.8754 ; Train_loss   0.3404\n",
      "epoch =  10 ; Train_acc :  0.8787 ; Train_loss   0.3345\n",
      "epoch =  11 ; Train_acc :  0.881 ; Train_loss   0.3266\n",
      "epoch =  12 ; Train_acc :  0.8817 ; Train_loss   0.3215\n",
      "epoch =  13 ; Train_acc :  0.8813 ; Train_loss   0.3231\n",
      "epoch =  14 ; Train_acc :  0.8837 ; Train_loss   0.3174\n",
      "epoch =  15 ; Train_acc :  0.8846 ; Train_loss   0.312\n",
      "epoch =  16 ; Train_acc :  0.8861 ; Train_loss   0.3081\n",
      "epoch =  17 ; Train_acc :  0.887 ; Train_loss   0.3096\n",
      "epoch =  18 ; Train_acc :  0.889 ; Train_loss   0.3038\n",
      "epoch =  19 ; Train_acc :  0.8895 ; Train_loss   0.3038\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 20 epochs on full training set\n",
    "#Create Model\n",
    "Model = MLP(784,10,d_out=True, b_norm=False).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, aug=False, validate=False, max_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.3248896585485806\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.840864   0.856  0.848365\n",
      "1      1   0.992783   0.963  0.977665\n",
      "2      2   0.790402   0.807  0.798615\n",
      "3      3   0.841155   0.932  0.884250\n",
      "4      4   0.806218   0.778  0.791858\n",
      "5      5   0.985656   0.962  0.973684\n",
      "6      6   0.731148   0.669  0.698695\n",
      "7      7   0.934223   0.980  0.956564\n",
      "8      8   0.977000   0.977  0.977000\n",
      "9      9   0.971370   0.950  0.960566\n",
      "\n",
      "Accuracy  :  0.8873999714851379\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/dropout_MLP.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.8245 ; Val_acc :  0.8518 ; Train_loss   0.4882 ; Val_loss   0.4087\n",
      "epoch :  1 ; Train_acc :  0.8539 ; Val_acc :  0.857 ; Train_loss   0.3994 ; Val_loss   0.4011\n",
      "epoch :  2 ; Train_acc :  0.8662 ; Val_acc :  0.8714 ; Train_loss   0.3645 ; Val_loss   0.3682\n",
      "epoch :  3 ; Train_acc :  0.8762 ; Val_acc :  0.879 ; Train_loss   0.3375 ; Val_loss   0.3562\n",
      "epoch :  4 ; Train_acc :  0.8847 ; Val_acc :  0.8582 ; Train_loss   0.3174 ; Val_loss   0.4786\n",
      "epoch :  5 ; Train_acc :  0.8883 ; Val_acc :  0.8783 ; Train_loss   0.303 ; Val_loss   0.3546\n",
      "epoch :  6 ; Train_acc :  0.8913 ; Val_acc :  0.8803 ; Train_loss   0.2912 ; Val_loss   0.3612\n",
      "epoch :  7 ; Train_acc :  0.8947 ; Val_acc :  0.8883 ; Train_loss   0.282 ; Val_loss   0.345\n",
      "epoch :  8 ; Train_acc :  0.8977 ; Val_acc :  0.8813 ; Train_loss   0.275 ; Val_loss   0.3997\n",
      "epoch :  9 ; Train_acc :  0.9017 ; Val_acc :  0.8833 ; Train_loss   0.2644 ; Val_loss   0.405\n",
      "epoch :  10 ; Train_acc :  0.904 ; Val_acc :  0.8861 ; Train_loss   0.2555 ; Val_loss   0.3812\n",
      "epoch :  11 ; Train_acc :  0.9069 ; Val_acc :  0.8865 ; Train_loss   0.2505 ; Val_loss   0.3972\n",
      "epoch :  12 ; Train_acc :  0.91 ; Val_acc :  0.878 ; Train_loss   0.2377 ; Val_loss   0.4199\n",
      "epoch :  13 ; Train_acc :  0.9115 ; Val_acc :  0.8767 ; Train_loss   0.2358 ; Val_loss   0.383\n",
      "epoch :  14 ; Train_acc :  0.9146 ; Val_acc :  0.8846 ; Train_loss   0.2287 ; Val_loss   0.4894\n",
      "epoch :  15 ; Train_acc :  0.9154 ; Val_acc :  0.8879 ; Train_loss   0.2225 ; Val_loss   0.3542\n",
      "epoch :  16 ; Train_acc :  0.9177 ; Val_acc :  0.8898 ; Train_loss   0.2172 ; Val_loss   0.3634\n",
      "epoch :  17 ; Train_acc :  0.9184 ; Val_acc :  0.8922 ; Train_loss   0.2109 ; Val_loss   0.3515\n",
      "epoch :  18 ; Train_acc :  0.9231 ; Val_acc :  0.8886 ; Train_loss   0.2041 ; Val_loss   0.4236\n",
      "epoch :  19 ; Train_acc :  0.9237 ; Val_acc :  0.8942 ; Train_loss   0.2005 ; Val_loss   0.4334\n",
      "epoch :  20 ; Train_acc :  0.924 ; Val_acc :  0.8927 ; Train_loss   0.2013 ; Val_loss   0.3587\n",
      "epoch :  21 ; Train_acc :  0.9279 ; Val_acc :  0.891 ; Train_loss   0.1929 ; Val_loss   0.3878\n",
      "epoch :  22 ; Train_acc :  0.9282 ; Val_acc :  0.8842 ; Train_loss   0.1902 ; Val_loss   0.5349\n",
      "epoch :  23 ; Train_acc :  0.93 ; Val_acc :  0.8903 ; Train_loss   0.1852 ; Val_loss   0.4316\n",
      "epoch :  24 ; Train_acc :  0.9305 ; Val_acc :  0.8913 ; Train_loss   0.1811 ; Val_loss   0.3627\n",
      "epoch :  25 ; Train_acc :  0.9307 ; Val_acc :  0.8741 ; Train_loss   0.1799 ; Val_loss   0.5566\n",
      "epoch :  26 ; Train_acc :  0.934 ; Val_acc :  0.8884 ; Train_loss   0.173 ; Val_loss   0.3515\n",
      "epoch :  27 ; Train_acc :  0.9351 ; Val_acc :  0.8866 ; Train_loss   0.1717 ; Val_loss   0.4098\n",
      "epoch :  28 ; Train_acc :  0.937 ; Val_acc :  0.8916 ; Train_loss   0.1666 ; Val_loss   0.3492\n",
      "epoch :  29 ; Train_acc :  0.9376 ; Val_acc :  0.8948 ; Train_loss   0.1643 ; Val_loss   0.4094\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = MLP(784,10,d_out=False, b_norm=True).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, aug=False, validate=True, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.8264 ; Train_loss   0.4778\n",
      "epoch =  1 ; Train_acc :  0.8572 ; Train_loss   0.3894\n",
      "epoch =  2 ; Train_acc :  0.8702 ; Train_loss   0.356\n",
      "epoch =  3 ; Train_acc :  0.8765 ; Train_loss   0.333\n",
      "epoch =  4 ; Train_acc :  0.8806 ; Train_loss   0.3187\n",
      "epoch =  5 ; Train_acc :  0.8866 ; Train_loss   0.3039\n",
      "epoch =  6 ; Train_acc :  0.8911 ; Train_loss   0.2911\n",
      "epoch =  7 ; Train_acc :  0.8944 ; Train_loss   0.2816\n",
      "epoch =  8 ; Train_acc :  0.8964 ; Train_loss   0.2762\n",
      "epoch =  9 ; Train_acc :  0.9013 ; Train_loss   0.2622\n",
      "epoch =  10 ; Train_acc :  0.9044 ; Train_loss   0.2546\n",
      "epoch =  11 ; Train_acc :  0.9076 ; Train_loss   0.2452\n",
      "epoch =  12 ; Train_acc :  0.9102 ; Train_loss   0.242\n",
      "epoch =  13 ; Train_acc :  0.9118 ; Train_loss   0.2333\n",
      "epoch =  14 ; Train_acc :  0.9137 ; Train_loss   0.2296\n",
      "epoch =  15 ; Train_acc :  0.9157 ; Train_loss   0.2229\n",
      "epoch =  16 ; Train_acc :  0.9182 ; Train_loss   0.2175\n",
      "epoch =  17 ; Train_acc :  0.9191 ; Train_loss   0.2162\n",
      "epoch =  18 ; Train_acc :  0.9206 ; Train_loss   0.2137\n",
      "epoch =  19 ; Train_acc :  0.9211 ; Train_loss   0.2081\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 20 epochs on full training set\n",
    "#Create Model\n",
    "Model = MLP(784,10,d_out=False, b_norm=True).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, aug=False, validate=False, max_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.5158356818290183\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.883333   0.689  0.774157\n",
      "1      1   0.979021   0.980  0.979510\n",
      "2      2   0.817021   0.768  0.791753\n",
      "3      3   0.890547   0.895  0.892768\n",
      "4      4   0.823775   0.790  0.806534\n",
      "5      5   0.968464   0.952  0.960161\n",
      "6      6   0.585926   0.791  0.673191\n",
      "7      7   0.958244   0.895  0.925543\n",
      "8      8   0.980352   0.948  0.963905\n",
      "9      9   0.901018   0.974  0.936088\n",
      "\n",
      "Accuracy  :  0.8682000041007996\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/dropout_MLP.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.7573 ; Val_acc :  0.8282 ; Train_loss   0.6428 ; Val_loss   0.4574\n",
      "epoch :  1 ; Train_acc :  0.8132 ; Val_acc :  0.8429 ; Train_loss   0.4953 ; Val_loss   0.4292\n",
      "epoch :  2 ; Train_acc :  0.8276 ; Val_acc :  0.8565 ; Train_loss   0.4549 ; Val_loss   0.3765\n",
      "epoch :  3 ; Train_acc :  0.8387 ; Val_acc :  0.8624 ; Train_loss   0.4245 ; Val_loss   0.3729\n",
      "epoch :  4 ; Train_acc :  0.8441 ; Val_acc :  0.8668 ; Train_loss   0.4105 ; Val_loss   0.3548\n",
      "epoch :  5 ; Train_acc :  0.8512 ; Val_acc :  0.8677 ; Train_loss   0.3941 ; Val_loss   0.3526\n",
      "epoch :  6 ; Train_acc :  0.8536 ; Val_acc :  0.8744 ; Train_loss   0.3838 ; Val_loss   0.3423\n",
      "epoch :  7 ; Train_acc :  0.8582 ; Val_acc :  0.8795 ; Train_loss   0.3721 ; Val_loss   0.3222\n",
      "epoch :  8 ; Train_acc :  0.8621 ; Val_acc :  0.8746 ; Train_loss   0.3647 ; Val_loss   0.3312\n",
      "epoch :  9 ; Train_acc :  0.863 ; Val_acc :  0.8778 ; Train_loss   0.3598 ; Val_loss   0.327\n",
      "epoch :  10 ; Train_acc :  0.8657 ; Val_acc :  0.88 ; Train_loss   0.354 ; Val_loss   0.3225\n",
      "epoch :  11 ; Train_acc :  0.8677 ; Val_acc :  0.8789 ; Train_loss   0.3445 ; Val_loss   0.335\n",
      "epoch :  12 ; Train_acc :  0.8706 ; Val_acc :  0.8792 ; Train_loss   0.3412 ; Val_loss   0.3196\n",
      "epoch :  13 ; Train_acc :  0.8711 ; Val_acc :  0.8836 ; Train_loss   0.3405 ; Val_loss   0.3074\n",
      "epoch :  14 ; Train_acc :  0.8736 ; Val_acc :  0.8846 ; Train_loss   0.3307 ; Val_loss   0.31\n",
      "epoch :  15 ; Train_acc :  0.8751 ; Val_acc :  0.8879 ; Train_loss   0.3326 ; Val_loss   0.311\n",
      "epoch :  16 ; Train_acc :  0.8765 ; Val_acc :  0.8837 ; Train_loss   0.3263 ; Val_loss   0.3177\n",
      "epoch :  17 ; Train_acc :  0.8797 ; Val_acc :  0.8831 ; Train_loss   0.3174 ; Val_loss   0.3292\n",
      "epoch :  18 ; Train_acc :  0.8766 ; Val_acc :  0.8782 ; Train_loss   0.3233 ; Val_loss   0.3313\n",
      "epoch :  19 ; Train_acc :  0.8799 ; Val_acc :  0.8808 ; Train_loss   0.3157 ; Val_loss   0.3183\n",
      "epoch :  20 ; Train_acc :  0.8815 ; Val_acc :  0.8903 ; Train_loss   0.3122 ; Val_loss   0.2982\n",
      "epoch :  21 ; Train_acc :  0.8835 ; Val_acc :  0.8935 ; Train_loss   0.308 ; Val_loss   0.3115\n",
      "epoch :  22 ; Train_acc :  0.8837 ; Val_acc :  0.8809 ; Train_loss   0.3039 ; Val_loss   0.3469\n",
      "epoch :  23 ; Train_acc :  0.8846 ; Val_acc :  0.8887 ; Train_loss   0.3028 ; Val_loss   0.3119\n",
      "epoch :  24 ; Train_acc :  0.8853 ; Val_acc :  0.8886 ; Train_loss   0.3007 ; Val_loss   0.3171\n",
      "epoch :  25 ; Train_acc :  0.8852 ; Val_acc :  0.8864 ; Train_loss   0.3007 ; Val_loss   0.3232\n",
      "epoch :  26 ; Train_acc :  0.8861 ; Val_acc :  0.8896 ; Train_loss   0.3003 ; Val_loss   0.3201\n",
      "epoch :  27 ; Train_acc :  0.8885 ; Val_acc :  0.8906 ; Train_loss   0.294 ; Val_loss   0.3156\n",
      "epoch :  28 ; Train_acc :  0.8888 ; Val_acc :  0.8824 ; Train_loss   0.2932 ; Val_loss   0.3292\n",
      "epoch :  29 ; Train_acc :  0.8894 ; Val_acc :  0.8913 ; Train_loss   0.2926 ; Val_loss   0.3304\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = MLP(784,10,d_out=False, b_norm=False).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, aug=True, validate=True, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.7641 ; Train_loss   0.6254\n",
      "epoch =  1 ; Train_acc :  0.8168 ; Train_loss   0.484\n",
      "epoch =  2 ; Train_acc :  0.835 ; Train_loss   0.4375\n",
      "epoch =  3 ; Train_acc :  0.8428 ; Train_loss   0.4133\n",
      "epoch =  4 ; Train_acc :  0.8483 ; Train_loss   0.4001\n",
      "epoch =  5 ; Train_acc :  0.8515 ; Train_loss   0.3897\n",
      "epoch =  6 ; Train_acc :  0.8547 ; Train_loss   0.3794\n",
      "epoch =  7 ; Train_acc :  0.8605 ; Train_loss   0.3655\n",
      "epoch =  8 ; Train_acc :  0.8611 ; Train_loss   0.3637\n",
      "epoch =  9 ; Train_acc :  0.8662 ; Train_loss   0.353\n",
      "epoch =  10 ; Train_acc :  0.8686 ; Train_loss   0.3444\n",
      "epoch =  11 ; Train_acc :  0.8711 ; Train_loss   0.3381\n",
      "epoch =  12 ; Train_acc :  0.8722 ; Train_loss   0.3364\n",
      "epoch =  13 ; Train_acc :  0.8762 ; Train_loss   0.3287\n",
      "epoch =  14 ; Train_acc :  0.8754 ; Train_loss   0.3301\n",
      "epoch =  15 ; Train_acc :  0.876 ; Train_loss   0.3231\n",
      "epoch =  16 ; Train_acc :  0.877 ; Train_loss   0.3185\n",
      "epoch =  17 ; Train_acc :  0.8811 ; Train_loss   0.3152\n",
      "epoch =  18 ; Train_acc :  0.8803 ; Train_loss   0.3133\n",
      "epoch =  19 ; Train_acc :  0.8806 ; Train_loss   0.314\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 20 epochs on full training set\n",
    "#Create Model\n",
    "Model = MLP(784,10,d_out=False, b_norm=False).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, aug=True, validate=False, max_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.32735916778159596\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.810606   0.856  0.832685\n",
      "1      1   0.998962   0.962  0.980132\n",
      "2      2   0.855294   0.727  0.785946\n",
      "3      3   0.890909   0.882  0.886432\n",
      "4      4   0.761991   0.842  0.800000\n",
      "5      5   0.980553   0.958  0.969145\n",
      "6      6   0.692161   0.724  0.707722\n",
      "7      7   0.939806   0.968  0.953695\n",
      "8      8   0.987817   0.973  0.980353\n",
      "9      9   0.963928   0.962  0.962963\n",
      "\n",
      "Accuracy  :  0.8853999972343445\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/data_aug_MLP.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Dropout, BN and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 3 methods usually do nor perform well together as batch normalization usually eliminated the need for dropout. However, we can tune hyperparameterss to obtain good results. We can obtain better results by decreasing dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 ; Train_acc :  0.7386 ; Val_acc :  0.8102 ; Train_loss   0.7094 ; Val_loss   0.5185\n",
      "epoch :  1 ; Train_acc :  0.7706 ; Val_acc :  0.8279 ; Train_loss   0.6231 ; Val_loss   0.4622\n",
      "epoch :  2 ; Train_acc :  0.7808 ; Val_acc :  0.8253 ; Train_loss   0.5923 ; Val_loss   0.4695\n",
      "epoch :  3 ; Train_acc :  0.7851 ; Val_acc :  0.8374 ; Train_loss   0.5845 ; Val_loss   0.4527\n",
      "epoch :  4 ; Train_acc :  0.7928 ; Val_acc :  0.8424 ; Train_loss   0.5645 ; Val_loss   0.435\n",
      "epoch :  5 ; Train_acc :  0.7944 ; Val_acc :  0.8495 ; Train_loss   0.5546 ; Val_loss   0.4251\n",
      "epoch :  6 ; Train_acc :  0.7996 ; Val_acc :  0.8545 ; Train_loss   0.5431 ; Val_loss   0.3913\n",
      "epoch :  7 ; Train_acc :  0.8036 ; Val_acc :  0.851 ; Train_loss   0.5333 ; Val_loss   0.4454\n",
      "epoch :  8 ; Train_acc :  0.8023 ; Val_acc :  0.8396 ; Train_loss   0.5328 ; Val_loss   0.425\n",
      "epoch :  9 ; Train_acc :  0.8077 ; Val_acc :  0.8413 ; Train_loss   0.5262 ; Val_loss   0.4221\n",
      "epoch :  10 ; Train_acc :  0.8075 ; Val_acc :  0.8553 ; Train_loss   0.5213 ; Val_loss   0.3946\n",
      "epoch :  11 ; Train_acc :  0.812 ; Val_acc :  0.8503 ; Train_loss   0.5148 ; Val_loss   0.4019\n",
      "epoch :  12 ; Train_acc :  0.8135 ; Val_acc :  0.8458 ; Train_loss   0.502 ; Val_loss   0.409\n",
      "epoch :  13 ; Train_acc :  0.8128 ; Val_acc :  0.8555 ; Train_loss   0.5051 ; Val_loss   0.3869\n",
      "epoch :  14 ; Train_acc :  0.8131 ; Val_acc :  0.8563 ; Train_loss   0.504 ; Val_loss   0.3929\n",
      "epoch :  15 ; Train_acc :  0.8145 ; Val_acc :  0.8556 ; Train_loss   0.4994 ; Val_loss   0.3929\n",
      "epoch :  16 ; Train_acc :  0.8159 ; Val_acc :  0.861 ; Train_loss   0.496 ; Val_loss   0.3696\n",
      "epoch :  17 ; Train_acc :  0.8201 ; Val_acc :  0.8602 ; Train_loss   0.4858 ; Val_loss   0.3811\n",
      "epoch :  18 ; Train_acc :  0.8202 ; Val_acc :  0.855 ; Train_loss   0.4854 ; Val_loss   0.3998\n",
      "epoch :  19 ; Train_acc :  0.8197 ; Val_acc :  0.8543 ; Train_loss   0.485 ; Val_loss   0.3781\n",
      "epoch :  20 ; Train_acc :  0.8213 ; Val_acc :  0.8566 ; Train_loss   0.4805 ; Val_loss   0.3853\n",
      "epoch :  21 ; Train_acc :  0.8259 ; Val_acc :  0.8636 ; Train_loss   0.4742 ; Val_loss   0.362\n",
      "epoch :  22 ; Train_acc :  0.8257 ; Val_acc :  0.8661 ; Train_loss   0.4762 ; Val_loss   0.3597\n",
      "epoch :  23 ; Train_acc :  0.825 ; Val_acc :  0.8679 ; Train_loss   0.467 ; Val_loss   0.3592\n",
      "epoch :  24 ; Train_acc :  0.8243 ; Val_acc :  0.8603 ; Train_loss   0.4704 ; Val_loss   0.3787\n",
      "epoch :  25 ; Train_acc :  0.8243 ; Val_acc :  0.8639 ; Train_loss   0.4704 ; Val_loss   0.3738\n",
      "epoch :  26 ; Train_acc :  0.8259 ; Val_acc :  0.8621 ; Train_loss   0.4673 ; Val_loss   0.3757\n",
      "epoch :  27 ; Train_acc :  0.8273 ; Val_acc :  0.8606 ; Train_loss   0.4618 ; Val_loss   0.3729\n",
      "epoch :  28 ; Train_acc :  0.83 ; Val_acc :  0.866 ; Train_loss   0.4593 ; Val_loss   0.365\n",
      "epoch :  29 ; Train_acc :  0.828 ; Val_acc :  0.868 ; Train_loss   0.4623 ; Val_loss   0.3825\n",
      "epoch :  30 ; Train_acc :  0.8275 ; Val_acc :  0.867 ; Train_loss   0.4609 ; Val_loss   0.36\n",
      "epoch :  31 ; Train_acc :  0.8307 ; Val_acc :  0.8611 ; Train_loss   0.4557 ; Val_loss   0.367\n",
      "epoch :  32 ; Train_acc :  0.8309 ; Val_acc :  0.8676 ; Train_loss   0.4571 ; Val_loss   0.3527\n",
      "epoch :  33 ; Train_acc :  0.8328 ; Val_acc :  0.875 ; Train_loss   0.4501 ; Val_loss   0.3571\n",
      "epoch :  34 ; Train_acc :  0.8322 ; Val_acc :  0.8686 ; Train_loss   0.4552 ; Val_loss   0.3516\n",
      "epoch :  35 ; Train_acc :  0.8342 ; Val_acc :  0.8715 ; Train_loss   0.4517 ; Val_loss   0.3533\n",
      "epoch :  36 ; Train_acc :  0.8361 ; Val_acc :  0.8782 ; Train_loss   0.4417 ; Val_loss   0.3689\n",
      "epoch :  37 ; Train_acc :  0.835 ; Val_acc :  0.873 ; Train_loss   0.4457 ; Val_loss   0.3551\n",
      "epoch :  38 ; Train_acc :  0.8347 ; Val_acc :  0.8764 ; Train_loss   0.4442 ; Val_loss   0.3353\n",
      "epoch :  39 ; Train_acc :  0.8358 ; Val_acc :  0.8734 ; Train_loss   0.4401 ; Val_loss   0.3516\n",
      "epoch :  40 ; Train_acc :  0.8345 ; Val_acc :  0.8649 ; Train_loss   0.4483 ; Val_loss   0.3609\n",
      "epoch :  41 ; Train_acc :  0.8379 ; Val_acc :  0.8782 ; Train_loss   0.4391 ; Val_loss   0.3421\n",
      "epoch :  42 ; Train_acc :  0.8343 ; Val_acc :  0.8761 ; Train_loss   0.4462 ; Val_loss   0.3358\n",
      "epoch :  43 ; Train_acc :  0.8376 ; Val_acc :  0.8774 ; Train_loss   0.4386 ; Val_loss   0.3353\n",
      "epoch :  44 ; Train_acc :  0.8361 ; Val_acc :  0.872 ; Train_loss   0.4431 ; Val_loss   0.3434\n",
      "epoch :  45 ; Train_acc :  0.8363 ; Val_acc :  0.8758 ; Train_loss   0.4404 ; Val_loss   0.3308\n",
      "epoch :  46 ; Train_acc :  0.8375 ; Val_acc :  0.8745 ; Train_loss   0.4342 ; Val_loss   0.3324\n",
      "epoch :  47 ; Train_acc :  0.8377 ; Val_acc :  0.8732 ; Train_loss   0.4375 ; Val_loss   0.3324\n",
      "epoch :  48 ; Train_acc :  0.8401 ; Val_acc :  0.8756 ; Train_loss   0.4336 ; Val_loss   0.3327\n",
      "epoch :  49 ; Train_acc :  0.8372 ; Val_acc :  0.8738 ; Train_loss   0.4434 ; Val_loss   0.3871\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "Model = MLP(784,10,d_out=True, b_norm=True).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train model with validation\n",
    "train(Model, aug=True, validate=True, max_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create new model with lower dropout rate, to improve performance\n",
    "#Create a MLP class defining our neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_len, output_len, d_out, b_norm):\n",
    "        super(MLP, self).__init__()\n",
    "        #three fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=input_len, out_features=512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "        self.b_norm = b_norm\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #convert image to a one dimentional tensor before feeding to neural network\n",
    "        x = x.flatten(start_dim=1)\n",
    "        #activation function is relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if(self.b_norm):\n",
    "            x = self.bn1(x)\n",
    "        #dropout\n",
    "        if(self.d_out):\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if(self.b_norm):\n",
    "            x = self.bn2(x)\n",
    "        #dropout\n",
    "        if(self.d_out):\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 ; Train_acc :  0.7584 ; Train_loss   0.6517\n",
      "epoch =  1 ; Train_acc :  0.7901 ; Train_loss   0.5651\n",
      "epoch =  2 ; Train_acc :  0.802 ; Train_loss   0.5346\n",
      "epoch =  3 ; Train_acc :  0.8106 ; Train_loss   0.5101\n",
      "epoch =  4 ; Train_acc :  0.8147 ; Train_loss   0.4965\n",
      "epoch =  5 ; Train_acc :  0.8184 ; Train_loss   0.4826\n",
      "epoch =  6 ; Train_acc :  0.8248 ; Train_loss   0.4748\n",
      "epoch =  7 ; Train_acc :  0.8259 ; Train_loss   0.4699\n",
      "epoch =  8 ; Train_acc :  0.8288 ; Train_loss   0.4569\n",
      "epoch =  9 ; Train_acc :  0.8323 ; Train_loss   0.4508\n",
      "epoch =  10 ; Train_acc :  0.8324 ; Train_loss   0.4511\n",
      "epoch =  11 ; Train_acc :  0.8322 ; Train_loss   0.4479\n",
      "epoch =  12 ; Train_acc :  0.8348 ; Train_loss   0.4444\n",
      "epoch =  13 ; Train_acc :  0.8376 ; Train_loss   0.438\n",
      "epoch =  14 ; Train_acc :  0.8371 ; Train_loss   0.4342\n",
      "epoch =  15 ; Train_acc :  0.8409 ; Train_loss   0.4273\n",
      "epoch =  16 ; Train_acc :  0.8382 ; Train_loss   0.4314\n",
      "epoch =  17 ; Train_acc :  0.8401 ; Train_loss   0.4239\n",
      "epoch =  18 ; Train_acc :  0.8449 ; Train_loss   0.4186\n",
      "epoch =  19 ; Train_acc :  0.8423 ; Train_loss   0.4166\n",
      "epoch =  20 ; Train_acc :  0.8432 ; Train_loss   0.4167\n",
      "epoch =  21 ; Train_acc :  0.8458 ; Train_loss   0.4156\n",
      "epoch =  22 ; Train_acc :  0.8491 ; Train_loss   0.4047\n",
      "epoch =  23 ; Train_acc :  0.8432 ; Train_loss   0.4138\n",
      "epoch =  24 ; Train_acc :  0.8487 ; Train_loss   0.4066\n",
      "epoch =  25 ; Train_acc :  0.8478 ; Train_loss   0.404\n",
      "epoch =  26 ; Train_acc :  0.8489 ; Train_loss   0.4062\n",
      "epoch =  27 ; Train_acc :  0.851 ; Train_loss   0.3958\n",
      "epoch =  28 ; Train_acc :  0.8505 ; Train_loss   0.3965\n",
      "epoch =  29 ; Train_acc :  0.8508 ; Train_loss   0.3958\n"
     ]
    }
   ],
   "source": [
    "#Let's train our model for 30 epochs on full training set and by decresing dropout rate to 0.2\n",
    "#Create Model\n",
    "Model = MLP(784,10,d_out=True, b_norm=True).to(device)\n",
    "#Define optimizer\n",
    "optimizer = optim.Adam(Model.parameters())\n",
    "#train\n",
    "train(Model, aug=True, validate=False, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.3432034440219593\n",
      "\n",
      "    Class  Precision  Recall  F1_Score\n",
      "0      0   0.869823   0.735  0.796748\n",
      "1      1   0.979960   0.978  0.978979\n",
      "2      2   0.853051   0.685  0.759845\n",
      "3      3   0.847505   0.917  0.880884\n",
      "4      4   0.726974   0.884  0.797834\n",
      "5      5   0.975535   0.957  0.966179\n",
      "6      6   0.660360   0.698  0.678658\n",
      "7      7   0.915414   0.974  0.943798\n",
      "8      8   0.976861   0.971  0.973922\n",
      "9      9   0.975000   0.936  0.955102\n",
      "\n",
      "Accuracy  :  0.8734999895095825\n"
     ]
    }
   ],
   "source": [
    "#Let's test model now\n",
    "test(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's save our model\n",
    "torch.save(Model.state_dict(), './saved_models/Combined_MLP.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out different combinations of three techniques to get better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
